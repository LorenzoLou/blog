<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Domain Driven Design Practical Tutorial</title>
    <url>/blog/2022/05/30/DDD/</url>
    <content><![CDATA[<p>You can click on the <a href="https://lorenzolou.github.io/blog/">Blog Home</a> to browser my blogs and find more interesting<br>content. If there are any issues or suggestions, please feel free to reach out to me or simply submit a merge request<br>on <a href="https://github.com/LorenzoLou">GitHub Link</a></p>
<hr>
<h3 id="What’s-DDD"><a href="#What’s-DDD" class="headerlink" title="What’s DDD?"></a>What’s DDD?</h3><p>DDD(Domain Driven Design) is just a concept, an abstract instruction or direction to help us reduce the complexity of<br>our own application. But because it’s a concept, it has already confused so many developers. We need to implement the<br>concept by ourselves without any restrictions, which means you don’t have a tutorial sample to refer at the beginning.<br>Here we try to provide an implement an application architecture by using DDD concept, but before that, what’s<br>application Architecture?</p>
<h3 id="What’s-Application-Architecture"><a href="#What’s-Application-Architecture" class="headerlink" title="What’s Application Architecture"></a>What’s Application Architecture</h3><p>As we mentioned above, DDD is just a concept, but how we start to develop our own business logics by using this concept?<br>It’s a quite open question which actually does not have a fixed question. But from my perspective, I list the<br>requirements of Application Architecture:</p>
<ol>
<li>Application Architecture should cater kinds of team members in different levels by reduce the communication cost,<br>keep them in the same context, and help us to improve the code quality.</li>
<li>isolate our business codes by any dependencies, for instances the storage logics, the out system api dependencies.</li>
<li>every functionality should be independent and easy to test.</li>
</ol>
<p>here I will provide my own practical experience for you. Hopefully it can benefit you and help you generate your own<br>specific DDD implementations.</p>
<h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><h4 id="Scenario"><a href="#Scenario" class="headerlink" title="Scenario"></a>Scenario</h4><p>Let’s say we need to design a system to support users to transfer money.</p>
<h4 id="Development-Requirement-Analise"><a href="#Development-Requirement-Analise" class="headerlink" title="Development Requirement Analise"></a>Development Requirement Analise</h4><ul>
<li>we use h2 as the storage tech stack.</li>
<li>we need to think about the exchange rate.</li>
<li>we have some policy to protect our user from scam.</li>
<li>we can transfer money from accountA to accountB</li>
</ul>
<h4 id="implement-by-using-script-like-coding-style"><a href="#implement-by-using-script-like-coding-style" class="headerlink" title="implement by using script like coding style"></a>implement by using script like coding style</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public class TransferController &#123;</span><br><span class="line"></span><br><span class="line">    private TransferService transferService;</span><br><span class="line"></span><br><span class="line">    public Result&lt;Boolean&gt; transfer(String targetAccountNumber, BigDecimal amount, HttpSession session) &#123;</span><br><span class="line">        Long userId = (Long) session.getAttribute(&quot;userId&quot;);</span><br><span class="line">        return transferService.transfer(userId, targetAccountNumber, amount, &quot;CNY&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public class TransferServiceImpl implements TransferService &#123;</span><br><span class="line"></span><br><span class="line">    private static final String TOPIC_AUDIT_LOG = &quot;TOPIC_AUDIT_LOG&quot;;</span><br><span class="line">    private AccountMapper accountDAO;</span><br><span class="line">    private KafkaTemplate&lt;String, String&gt; kafkaTemplate;</span><br><span class="line">    private YahooForexService yahooForex;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public Result&lt;Boolean&gt; transfer(Long sourceUserId, String targetAccountNumber, BigDecimal targetAmount, String targetCurrency) &#123;</span><br><span class="line">        // 1. 从数据库读取数据，忽略所有校验逻辑如账号是否存在等</span><br><span class="line">        AccountDO sourceAccountDO = accountDAO.selectByUserId(sourceUserId);</span><br><span class="line">        AccountDO targetAccountDO = accountDAO.selectByAccountNumber(targetAccountNumber);</span><br><span class="line"></span><br><span class="line">        // 2. 业务参数校验</span><br><span class="line">        if (!targetAccountDO.getCurrency().equals(targetCurrency)) &#123;</span><br><span class="line">            throw new InvalidCurrencyException();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // 3. 获取外部数据，并且包含一定的业务逻辑</span><br><span class="line">        // exchange rate = 1 source currency = X target currency</span><br><span class="line">        BigDecimal exchangeRate = BigDecimal.ONE;</span><br><span class="line">        if (sourceAccountDO.getCurrency().equals(targetCurrency)) &#123;</span><br><span class="line">            exchangeRate = yahooForex.getExchangeRate(sourceAccountDO.getCurrency(), targetCurrency);</span><br><span class="line">        &#125;</span><br><span class="line">        BigDecimal sourceAmount = targetAmount.divide(exchangeRate, RoundingMode.DOWN);</span><br><span class="line"></span><br><span class="line">        // 4. 业务参数校验</span><br><span class="line">        if (sourceAccountDO.getAvailable().compareTo(sourceAmount) &lt; 0) &#123;</span><br><span class="line">            throw new InsufficientFundsException();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        if (sourceAccountDO.getDailyLimit().compareTo(sourceAmount) &lt; 0) &#123;</span><br><span class="line">            throw new DailyLimitExceededException();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // 5. 计算新值，并且更新字段</span><br><span class="line">        BigDecimal newSource = sourceAccountDO.getAvailable().subtract(sourceAmount);</span><br><span class="line">        BigDecimal newTarget = targetAccountDO.getAvailable().add(targetAmount);</span><br><span class="line">        sourceAccountDO.setAvailable(newSource);</span><br><span class="line">        targetAccountDO.setAvailable(newTarget);</span><br><span class="line"></span><br><span class="line">        // 6. 更新到数据库</span><br><span class="line">        accountDAO.update(sourceAccountDO);</span><br><span class="line">        accountDAO.update(targetAccountDO);</span><br><span class="line"></span><br><span class="line">        // 7. 发送审计消息</span><br><span class="line">        String message = sourceUserId + &quot;,&quot; + targetAccountNumber + &quot;,&quot; + targetAmount + &quot;,&quot; + targetCurrency;</span><br><span class="line">        kafkaTemplate.send(TOPIC_AUDIT_LOG, message);</span><br><span class="line"></span><br><span class="line">        return Result.success(true);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>As we can see this epitomise the typical three layer development approach. But actually there are several big downsides<br>about this type of development.</p>
<ul>
<li>Cost of the maintenance of the system is poor. Every time you’re going to change the verification logics or exchange<br>rate logics or something, you have to update this part of code that involves the parameter verification, storage,<br>compute, calling external service, and so on. For example if we wanna sharding the tables, it will be a disaster.</li>
<li>The scalability of this system is bad. If we wanna add a new transfer function which happens between different bank,<br>none of the codes can be reused.</li>
<li>Very difficult to develop the unit test cases. In this assumption, one method contains the dependency of db, external<br>system, and we need to mock each of them if we just wanna add a unit test case focused on our own business logics.</li>
</ul>
<h3 id="Why"><a href="#Why" class="headerlink" title="Why?"></a>Why?</h3><p>Why we encountered these issues? the reason behind it is the typical three layer architecture breaks several design<br>principles:</p>
<ul>
<li>Single Responsibility Principle. Let’s say we need to do a sharding to one table, we need to update all the logics<br>related to the dao, do in the service method. That’s a violation of Single Responsibility Principle.</li>
<li>Dependency Inversion Principle. You can see our service depend on the implements of db and external service. Once we<br>wanna do some updates of the db or external service, we have to go through the business service code.</li>
<li>Open Closed Principle. If we wanna add some other logics, for instance bank need to charge for a transfer fee. we have<br>to modify the service code rather than add code.</li>
</ul>
<h3 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h3><p>The main objective of our architecture is we can conform the Dependency Inversion Principle. All the database<br>implementations, all the api implementations, all the web api will depend on the kernel layer(domain). But how we<br>implement it in our case?</p>
<p><img src="/blog/2022/05/30/DDD/Architecture.png"></p>
<p>As we can see, we can start develop from domain layer, all the business logics will depend on the abstractions rather<br>than the specific impleme ntations. The most basic dependency is no longer the database, it’s Domain Layer. Domain Layer<br>doesn’t have any dependency on other external dependencies. For instance, we don’t need to think about what kind of<br>database we need to use, how to design the interaction ways with other external systems when we start to develop our<br>system. All these services will depend on the abstractions(ACL classes and Repository classes defined in domain layer).</p>
<p>From this perspective, web controller, rpc service, databases, external api provided by other systems are all at the<br>same level, all of them depend on the abstractions defined by domain layer.</p>
<h4 id="Infrastructure-Layer"><a href="#Infrastructure-Layer" class="headerlink" title="Infrastructure Layer"></a>Infrastructure Layer</h4><p>First, Let’s have a look at the Infrastructure Layer. The implementation(AccountRepositoryImpl) will depend on the<br>AccountRepository defined in Domain Layer. So when we develop our domain business logics, there is no need for us to<br>think how we communicate with database. Relatively speaking when we development how to implement the<br>AccountRepositoryImpl, we also just need to focus on how to store the domain object(Account).</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@Component</span><br><span class="line">@RequiredArgsConstructor</span><br><span class="line">public class AccountRepositoryImpl implements AccountRepository &#123;</span><br><span class="line">    private final InfrastructureConverter infrastructureConverter;</span><br><span class="line">    private final AccountDao accountDao;</span><br><span class="line">    private final BalanceDao balanceDao;</span><br><span class="line">    private final CustomerDao customerDao;</span><br><span class="line">    private final CurrencySystem currencySystem;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public Long store(Account account) &#123;</span><br><span class="line">        AccountDO accountDO = infrastructureConverter.copy(account);</span><br><span class="line">        CustomerDO customerDO = customerDao.save(accountDO.getCustomerDO());</span><br><span class="line">        BalanceDO balanceDO = balanceDao.save(accountDO.getBalanceDO());</span><br><span class="line">        accountDO.setCustomerDO(customerDO);</span><br><span class="line">        accountDO.setBalanceDO(balanceDO);</span><br><span class="line">        return accountDao.save(accountDO).getAccountNumber();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public Account load(Long id) &#123;</span><br><span class="line">        Optional&lt;AccountDO&gt; accountDOOptional = accountDao.findById(id);</span><br><span class="line">        if (accountDOOptional.isEmpty()) &#123;</span><br><span class="line">            return null;</span><br><span class="line">        &#125;</span><br><span class="line">        AccountDO accountDO = accountDOOptional.get();</span><br><span class="line">        Account account = infrastructureConverter.copy(accountDO);</span><br><span class="line">        Customer customer = infrastructureConverter.copy(accountDO.getCustomerDO());</span><br><span class="line">        Money money = infrastructureConverter.copy(accountDO.getBalanceDO());</span><br><span class="line">        account.setBalance(money);</span><br><span class="line">        account.setCustomer(customer);</span><br><span class="line">        account.setCurrencySystem(currencySystem);</span><br><span class="line">        return account;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h4 id="Anti-Corruption-Layer"><a href="#Anti-Corruption-Layer" class="headerlink" title="Anti Corruption Layer"></a>Anti Corruption Layer</h4><p>Besides, we also put ACL in this Infrastructure Layer, once again, the only thing we need to do is focusing on the<br>domain logics. So in Anti Corruption Layer, we need to convert the objects from external system to the objects defined<br>in our own system.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@Component</span><br><span class="line">public class MockCurrencySystem implements CurrencySystem &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public Money toDollar(Money money) &#123;</span><br><span class="line">        Float convertedAmount;</span><br><span class="line">        switch (money.getCurrencyType()) &#123;</span><br><span class="line">            case RMB: convertedAmount = money.getAmount() * 7.8F; break;</span><br><span class="line">            case USD: convertedAmount = money.getAmount(); break;</span><br><span class="line">            default: throw new RuntimeException(&quot;unrecognised currency&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        return new Money(CurrencyType.USD, convertedAmount);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="Domain-Layer"><a href="#Domain-Layer" class="headerlink" title="Domain Layer"></a>Domain Layer</h4><p>In the domain layer, we abstract all the business logics into the domain object.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@AllArgsConstructor</span><br><span class="line">public class Account &#123;</span><br><span class="line">    @Getter</span><br><span class="line">    @Setter</span><br><span class="line">    private Long accountNumber;</span><br><span class="line"></span><br><span class="line">    @Getter</span><br><span class="line">    @Setter</span><br><span class="line">    private Customer customer;</span><br><span class="line"></span><br><span class="line">    // to simplify this scenario, Let&#x27;s assume that all bank account money is denominated in USD</span><br><span class="line">    @Getter</span><br><span class="line">    @Setter</span><br><span class="line">    private Money balance;</span><br><span class="line"></span><br><span class="line">    @Setter</span><br><span class="line">    private CurrencySystem currencySystem;</span><br><span class="line"></span><br><span class="line">    @SneakyThrows</span><br><span class="line">    public void withDraw(Money money) &#123;</span><br><span class="line">        Money withDrawMoney = currencySystem.toDollar(money);</span><br><span class="line">        if (balance.getAmount() &gt;= withDrawMoney.getAmount()) &#123;</span><br><span class="line">            balance.setAmount(balance.getAmount() - withDrawMoney.getAmount());</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            throw new Exception(&quot;no enough money&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void deposit(Money money) &#123;</span><br><span class="line">        Money depositMoney = currencySystem.toDollar(money);</span><br><span class="line">        balance.setAmount(balance.getAmount() + depositMoney.getAmount());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>In domain service, we abstract all the business logics that not suitable to put in the entity&#x2F;aggregate object. Here we<br>put all the Limit Policies into the domain service. And suppose someday we need to add more limitStrategy, there is no<br>need for us to update the code. All we need to do is adding code.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@Service</span><br><span class="line">@RequiredArgsConstructor</span><br><span class="line">public class AccountDomainService &#123;</span><br><span class="line">    private final List&lt;LimitStrategy&gt; limitStrategyList;</span><br><span class="line"></span><br><span class="line">    public void transferMoney(Account source, Account dest, Money money) &#123;</span><br><span class="line">        for (LimitStrategy limitStrategy : limitStrategyList) &#123;</span><br><span class="line">            limitStrategy.allowable(source, dest, money);</span><br><span class="line">        &#125;</span><br><span class="line">        source.withDraw(money);</span><br><span class="line">        dest.deposit(money);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Application-Layer"><a href="#Application-Layer" class="headerlink" title="Application Layer"></a>Application Layer</h4><p>In the application layer, we can find that the ApplicationService will only coordinate the domain objects and domain<br>service to implement the business logics. And it’s so clean and easy for us to meet other requirements by<br>re-coordinating the domain objects.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@Slf4j</span><br><span class="line">@Service</span><br><span class="line">@RequiredArgsConstructor</span><br><span class="line">public class AccountApplicationService &#123;</span><br><span class="line">    private final AccountDomainService accountDomainService;</span><br><span class="line">    private final AccountRepository accountRepository;</span><br><span class="line">    private final AccountFactory accountFactory;</span><br><span class="line"></span><br><span class="line">    @SneakyThrows</span><br><span class="line">    @Transactional</span><br><span class="line">    public void transferMoney(Long sourceAccountNumber, Long destAccountNumber, Money amount) &#123;</span><br><span class="line">        Account source = accountRepository.load(sourceAccountNumber);</span><br><span class="line">        Account dest = accountRepository.load(destAccountNumber);</span><br><span class="line">        if (source == null || dest == null) &#123;</span><br><span class="line">            log.error(&quot;error account number: source &#123;&#125;, dest: &#123;&#125;, amount &#123;&#125;&quot;,</span><br><span class="line">                    sourceAccountNumber, destAccountNumber, amount);</span><br><span class="line">            throw new Exception(&quot;invalid account number&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        accountDomainService.transferMoney(source, dest, amount);</span><br><span class="line">        accountRepository.store(source);</span><br><span class="line">        accountRepository.store(dest);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void createAccount(AccountDTO accountDTO) &#123;</span><br><span class="line">        Account account = accountFactory.newAccount(accountDTO);</span><br><span class="line">        accountRepository.store(account);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Unit-Test"><a href="#Unit-Test" class="headerlink" title="Unit Test"></a>Unit Test</h4><ul>
<li>test cases in the Domain Layer, are very easy to develop. we just need to mock the external system results.</li>
<li>test cases in the Infrastructure Layer, are also easy for us to develop. we can use test database like h2, and we can<br>also take advantage of JPA test framework to make our test case development easier.</li>
<li>test cases in Biz Layer, can focus on the coordinate work rather than every business approaches. Practically we just<br>mock several cases to make sure that we can cover the application service.</li>
<li>test cases in Adaptor Layer. We can use spring mvc test tools to test the Adaptors.</li>
</ul>
<p>Ideally, each point above we can reach out to 100% coverage.</p>
<h4 id="System-Structure"><a href="#System-Structure" class="headerlink" title="System Structure"></a>System Structure</h4><p>This is the screenshot of our system’s packages.<br><img src="/blog/2022/05/30/DDD/adaptor.png"><br><img src="/blog/2022/05/30/DDD/biz.png"><br><img src="/blog/2022/05/30/DDD/infrastructure.png"><br><img src="/blog/2022/05/30/DDD/domain.png"></p>
<p>And actually we borrowed some good ideas from Clean Architecture, domain layer is located in the right middle of the<br>circal, Coordinate work, application service wraps the domain layer, and provide service to the Adapter Layer(<br>web&#x2F;api&#x2F;rpc). There is a special point, the infrastructure layer will depend on the domain directly.</p>
<p><img src="/blog/2022/05/30/DDD/clean-architecture.png"></p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>Let’s say someday we need to add transaction fee when user transfer money to another account. So first thing come up to<br>our mind is we need to modify some code in our domain layer, maybe we need to add some entity, maybe we need to create<br>another dependency on other system, but the most important thing is, we start to think from the domain layer, and then<br>we think about how to implement the abstraction defined by domain layer, how to save the status of domain objects. The<br>way how we develop, is the very proof that why we call it “driven by the domain”.</p>
]]></content>
  </entry>
  <entry>
    <title>Linux Virtual Memory Introduction</title>
    <url>/blog/2023/03/25/Linux-Virtual-Memory-Introduction/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Explaination of the principle of Oauth2.0</title>
    <url>/blog/2022/05/30/Principle-of-OAuth2-0/</url>
    <content><![CDATA[<h2 id="You-can-click-on-the-Blog-Home-to-browser-my-blogs-and-find-more-interesting-content-If-there-are-anyissues-or-suggestions-please-feel-free-to-reach-out-to-me-or-simply-submit-a-merge-request-onGitHub-Link"><a href="#You-can-click-on-the-Blog-Home-to-browser-my-blogs-and-find-more-interesting-content-If-there-are-anyissues-or-suggestions-please-feel-free-to-reach-out-to-me-or-simply-submit-a-merge-request-onGitHub-Link" class="headerlink" title="You can click on the Blog Home to browser my blogs and find more interesting content. If there are anyissues or suggestions, please feel free to reach out to me or simply submit a merge request onGitHub Link"></a>You can click on the <a href="../index.md">Blog Home</a> to browser my blogs and find more interesting content. If there are any<br>issues or suggestions, please feel free to reach out to me or simply submit a merge request on<br><a href="https://lorenzolou.github.io/blog/">GitHub Link</a></h2><h3 id="What’s-OAuth"><a href="#What’s-OAuth" class="headerlink" title="What’s OAuth?"></a>What’s OAuth?</h3><p>OAuth is a protocol that designed to protect the user who wanna share their data to third part application or systems.<br>OAuth 2.0 is subsequent version of OAuth 1.0, mostly we use OAuth 2.0 nowadays.</p>
<h3 id="Sample"><a href="#Sample" class="headerlink" title="Sample"></a>Sample</h3><p>Let’s say when we’re going to log in YOUTUBE by using the account of GOOGLE</p>
<h4 id="Terms"><a href="#Terms" class="headerlink" title="Terms"></a>Terms</h4><p>In this case some here are the professional introduction of terms we may use.</p>
<ul>
<li>Resource Server: In this case, YOUTUBE is going to get the username, profile, avatar from GOOGLE, so we call GOOGLE<br>service which stored this kind of user information as Resource Server</li>
<li>Resource Owner: Denotes the user here.</li>
<li>Client: Denotes YOUTUBE here.</li>
<li>Authorization Server: In this case, it should be the server of GOOGLE also.(we just differentiate Authorization Server<br>with Resource Service logically, they can be provided by a same server)</li>
<li>Useragent: The browser you use when you’re browsing the YOUTUBE videos.</li>
</ul>
<p>Base on authorization code mode</p>
<p><img src="/blog/2022/05/30/Principle-of-OAuth2-0/OAuth2.0-Diagram.drawio.svg"></p>
<p>if your application is based on frontend and backend separation architecture, then every request you send should involve<br>the token.</p>
<p>For more details<br>see <a href="https://docs.github.com/en/github/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax">Basic writing and formatting syntax</a><br>.</p>
<h3 id="AUTH-MODE"><a href="#AUTH-MODE" class="headerlink" title="AUTH MODE"></a>AUTH MODE</h3><p>We have 5 kinds of authorization modes.</p>
<ol>
<li>Authorization code</li>
<li>Implicit</li>
<li>Resource Owner Password Credentials</li>
<li>Client Credentials</li>
</ol>
<p>Authorization Code Mode is the most functional, secure and commonly used mode in our projects. So in our next blog we<br>prepared Resource Service, Auth Service, and Client in which we use Spring Security to implement the OAuth 2.0 protocol<br>and help us understand all the details related the OAuth 2.0.</p>
<p><a href="./Using%20Spring%20Security%20to%20implement%20OAuth2.0.md">How to implement OAuth by Using Spring Security Framework</a></p>
]]></content>
  </entry>
  <entry>
    <title>Zookeeper Election Mechanism Introduction</title>
    <url>/blog/2022/06/19/Zookeeper-Election/</url>
    <content><![CDATA[<p>You can click on the <a href="../index.md">Blog Home</a> to browser my blogs and find more interesting content. If there are any issues or suggestions, please feel free to reach out to me or simply submit a merge request on<br><a href="https://lorenzolou.github.io/blog/">GitHub Link</a></p>
<hr>
<h3 id="What’s-zookeeper"><a href="#What’s-zookeeper" class="headerlink" title="What’s zookeeper?"></a>What’s zookeeper?</h3><p>Zookeeper is a distributed system. In most cases, zookeeper acts as a coordinator, rather than a storage or message queue. For instance, let’s say we wanna inform other systems after we have done some stuffs, in this scerario, we can create a path as a signal on zabbix, and systems which want to know the events can subscribe the path event. It’s a powful coordinator providing data consistency services and most important as it is a distributed system, it also conform to the CAP rules.</p>
<h3 id="What’s-CAP"><a href="#What’s-CAP" class="headerlink" title="What’s CAP?"></a>What’s CAP?</h3><p>Theoretically, the CAP theorem, also known as Brewer’s theorem, states that it is impossible for a distributed computing system to satisfy the following three points at the same time: </p>
<ul>
<li>Consistence (all nodes can access the same latest copy of data) </li>
<li>Availability (every request can get a non-error response in a certain period of time - but the data obtained is not guaranteed to be the latest data)</li>
<li>Partition fault tolerance (Network partitioning) (System can always keep in consistency in a certain period of time. For instance, the network split into two regions, System can only choose to provide the service but with inconsistent data or stop providing service to keep the data consistent. That means If the node on one side of the partition is set to be unavailable in order to keep data consistency, the item A will fail. If two nodes can communicate with each other, both C and A can be guaranteed, but this leads to the loss of the Item C.)</li>
</ul>
<h3 id="How-zookeeper-keep-data-consistency"><a href="#How-zookeeper-keep-data-consistency" class="headerlink" title="How zookeeper keep data consistency?"></a>How zookeeper keep data consistency?</h3><p>Before we discuss of the data consistency, we must understand the principle of zookeeper’s election. Here is some specialist terms:</p>
<ul>
<li>Leader, responsible for all the transaction requests, like data update, data add, and so forth.</li>
<li>Follower, responsible for the non-transaction requests, like data query.</li>
<li>ZXID, ZooKeeper Transaction Id, denotes the latest data version one node has.</li>
</ul>
<p>Let’s assume we have three nodes, only if we elect the leader then the cluster can continue to work. So how they elect the leader?</p>
<ol>
<li>Choose a node with the latest data version(ZXID). When the election start, each node deem themself as the node with latest ZXID. They will sign their Identifier Card number (SID) on the vote and broadcast vote requests for themselves. Actually the reason why they gotta broadcast requests is every vode has their own vote ballot box.</li>
<li>It’s Kinda different from the way we human vote. Each node of the zabbix cluster mantain a ballot box. As each node will send their vote to all the nodes cluster has, so all the nodes will hold the same votes eventually.</li>
<li>They will try to find the one who holds the most votes will be the leader. Of cause there is a precondition, the number of votes leader hold must be more than half of the current cluster.</li>
</ol>
<h3 id="Zookeeper-Election"><a href="#Zookeeper-Election" class="headerlink" title="Zookeeper Election"></a>Zookeeper Election</h3><p>Let’s jump int the code. </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">   public interface Election &#123;</span><br><span class="line"></span><br><span class="line">    Vote lookForLeader() throws InterruptedException;</span><br><span class="line">    void shutdown();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>This <code>Election</code> interface is very important, zookeeper implement the interface by developed a class called <code>FastLeaderElection</code> , let’s jump into the makeOffer()</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Messages that a peer wants to send to other peers.</span><br><span class="line"> * These messages can be both Notifications and Acks</span><br><span class="line"> * of reception of notification.</span><br><span class="line"> */</span><br><span class="line">public static class ToSend &#123;...&#125;</span><br><span class="line"></span><br><span class="line">LinkedBlockingQueue&lt;ToSend&gt; sendqueue;</span><br><span class="line">LinkedBlockingQueue&lt;Notification&gt; recvqueue;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Multi-threaded implementation of message handler. Messenger</span><br><span class="line"> * implements two sub-classes: WorkReceiver and  WorkSender. The</span><br><span class="line"> * functionality of each is obvious from the name. Each of these</span><br><span class="line"> * spawns a new thread.</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">protected class Messenger &#123;...&#125;</span><br><span class="line"></span><br><span class="line">QuorumPeer self;</span><br><span class="line">Messenger messenger;</span><br><span class="line">AtomicLong logicalclock = new AtomicLong(); /* Election instance */</span><br><span class="line">long proposedLeader;</span><br><span class="line">long proposedZxid;</span><br><span class="line">long proposedEpoch;</span><br></pre></td></tr></table></figure>

<p>The most important thing is you can see that <code>FastLeaderElection</code> use the most typical Composite Pattern. It include <code>QuorumPeer</code>, which mainly involve all the core logics of ZK election. Let’s jump into <code>QuorumPeer</code> to investigate.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@Override</span><br><span class="line">public synchronized void start() &#123;</span><br><span class="line">    if (!getView().containsKey(myid)) &#123;</span><br><span class="line">        throw new RuntimeException(&quot;My id &quot; + myid + &quot; not in the peer list&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">    loadDataBase();</span><br><span class="line">    startServerCnxnFactory();</span><br><span class="line">    try &#123;</span><br><span class="line">        adminServer.start();</span><br><span class="line">    &#125; catch (AdminServerException e) &#123;</span><br><span class="line">        LOG.warn(&quot;Problem starting AdminServer&quot;, e);</span><br><span class="line">    &#125;</span><br><span class="line">    startLeaderElection();</span><br><span class="line">    startJvmPauseMonitor();</span><br><span class="line">    super.start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public synchronized void startLeaderElection() &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">        if (getPeerState() == ServerState.LOOKING) &#123;</span><br><span class="line">            currentVote = new Vote(myid, getLastLoggedZxid(), getCurrentEpoch());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; catch (IOException e) &#123;</span><br><span class="line">        RuntimeException re = new RuntimeException(e.getMessage());</span><br><span class="line">        re.setStackTrace(e.getStackTrace());</span><br><span class="line">        throw re;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    this.electionAlg = createElectionAlgorithm(electionType);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>actually in <code>createElectionAlgorithm</code>, it is also involves a <code>FastLeaderElection</code>. In case LOOKING, it contains all the logics which is responsible for the leader election. The method <code>totalOrderPredicate</code> is mainly responsible for judging which zknode is “Stronger” according to the <code>epoch</code>, <code>zkid</code>, <code>zid</code>.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">case LOOKING:</span><br><span class="line">    if (getInitLastLoggedZxid() == -1) &#123;</span><br><span class="line">        LOG.debug(&quot;Ignoring notification as our zxid is -1&quot;);</span><br><span class="line">        break;</span><br><span class="line">    &#125;</span><br><span class="line">    if (n.zxid == -1) &#123;</span><br><span class="line">        LOG.debug(&quot;Ignoring notification from member with -1 zxid &#123;&#125;&quot;, n.sid);</span><br><span class="line">        break;</span><br><span class="line">    &#125;</span><br><span class="line">    // If notification &gt; current, replace and send messages out</span><br><span class="line">    if (n.electionEpoch &gt; logicalclock.get()) &#123;</span><br><span class="line">        logicalclock.set(n.electionEpoch);</span><br><span class="line">        recvset.clear();</span><br><span class="line">        if (totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, getInitId(), getInitLastLoggedZxid(), getPeerEpoch())) &#123;</span><br><span class="line">            updateProposal(n.leader, n.zxid, n.peerEpoch);</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            updateProposal(getInitId(), getInitLastLoggedZxid(), getPeerEpoch());</span><br><span class="line">        &#125;</span><br><span class="line">        sendNotifications();</span><br><span class="line">    &#125; else if (n.electionEpoch &lt; logicalclock.get()) &#123;</span><br><span class="line">            LOG.debug(</span><br><span class="line">                &quot;Notification election epoch is smaller than logicalclock. n.electionEpoch = 0x&#123;&#125;, logicalclock=0x&#123;&#125;&quot;,</span><br><span class="line">                Long.toHexString(n.electionEpoch),</span><br><span class="line">                Long.toHexString(logicalclock.get()));</span><br><span class="line">        break;</span><br><span class="line">    &#125; else if (totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, proposedLeader, proposedZxid, proposedEpoch)) &#123;</span><br><span class="line">        updateProposal(n.leader, n.zxid, n.peerEpoch);</span><br><span class="line">        sendNotifications();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    LOG.debug(</span><br><span class="line">        &quot;Adding vote: from=&#123;&#125;, proposed leader=&#123;&#125;, proposed zxid=0x&#123;&#125;, proposed election epoch=0x&#123;&#125;&quot;,</span><br><span class="line">        n.sid,</span><br><span class="line">        n.leader,</span><br><span class="line">        Long.toHexString(n.zxid),</span><br><span class="line">        Long.toHexString(n.electionEpoch));</span><br><span class="line"></span><br><span class="line">    // don&#x27;t care about the version if it&#x27;s in LOOKING state</span><br><span class="line">    recvset.put(n.sid, new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch));</span><br><span class="line"></span><br><span class="line">    voteSet = getVoteTracker(recvset, new Vote(proposedLeader, proposedZxid, logicalclock.get(), proposedEpoch));</span><br><span class="line"></span><br><span class="line">    if (voteSet.hasAllQuorums()) &#123;</span><br><span class="line"></span><br><span class="line">        // Verify if there is any change in the proposed leader</span><br><span class="line">        while ((n = recvqueue.poll(finalizeWait, TimeUnit.MILLISECONDS)) != null) &#123;</span><br><span class="line">            if (totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, proposedLeader, proposedZxid, proposedEpoch)) &#123;</span><br><span class="line">                recvqueue.put(n);</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        /*</span><br><span class="line">         * This predicate is true once we don&#x27;t read any new</span><br><span class="line">         * relevant message from the reception queue</span><br><span class="line">         */</span><br><span class="line">        if (n == null) &#123;</span><br><span class="line">            setPeerState(proposedLeader, voteSet);</span><br><span class="line">            Vote endVote = new Vote(proposedLeader, proposedZxid, logicalclock.get(), proposedEpoch);</span><br><span class="line">            leaveInstance(endVote);</span><br><span class="line">            return endVote;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    break;</span><br></pre></td></tr></table></figure>

<p>Below is right part we refered to in the former section. </p>
<ol>
<li>if new epoch is higher, update vote as the new one.</li>
<li>if new epoch is the same as current epoch, but new zxid is higher, update vote as the new one.</li>
<li>if new epoch is the same as current epoch, new zxid is the same, update vote as the new one.</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Check if a pair (server id, zxid) succeeds our</span><br><span class="line"> * current vote.</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line">protected boolean totalOrderPredicate(long newId, long newZxid, long newEpoch, long curId, long curZxid, long curEpoch) &#123;</span><br><span class="line"></span><br><span class="line">    if (self.getQuorumVerifier().getWeight(newId) == 0) &#123;</span><br><span class="line">        return false;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return ((newEpoch &gt; curEpoch)</span><br><span class="line">            || ((newEpoch == curEpoch)</span><br><span class="line">                &amp;&amp; ((newZxid &gt; curZxid)</span><br><span class="line">                    || ((newZxid == curZxid)</span><br><span class="line">                        &amp;&amp; (newId &gt; curId)))));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>But how we confirm that the vote has already finished? The answer is time. First, node will judge if they already has all the votes come from other zknodes. Second, zk node will wait a specifical time for any more updated from others. If it doesn’t receive any updates, it will update peerStatus.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (voteSet.hasAllQuorums()) &#123;</span><br><span class="line"></span><br><span class="line">    // Verify if there is any change in the proposed leader</span><br><span class="line">    while ((n = recvqueue.poll(finalizeWait, TimeUnit.MILLISECONDS)) != null) &#123;</span><br><span class="line">        if (totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, proposedLeader, proposedZxid, proposedEpoch)) &#123;</span><br><span class="line">            recvqueue.put(n);</span><br><span class="line">            break;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">     * This predicate is true once we don&#x27;t read any new</span><br><span class="line">     * relevant message from the reception queue</span><br><span class="line">     */</span><br><span class="line">    if (n == null) &#123;</span><br><span class="line">        setPeerState(proposedLeader, voteSet);</span><br><span class="line">        Vote endVote = new Vote(proposedLeader, proposedZxid, logicalclock.get(), proposedEpoch);</span><br><span class="line">        leaveInstance(endVote);</span><br><span class="line">        return endVote;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>This logic is simple, if the current node is not leader, it will update the status to learningState, other vise it will update the status to LEADING.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Update the peer state based on the given proposedLeader. Also update</span><br><span class="line"> * the leadingVoteSet if it becomes the leader.</span><br><span class="line"> */</span><br><span class="line">private void setPeerState(long proposedLeader, SyncedLearnerTracker voteSet) &#123;</span><br><span class="line">    ServerState ss = (proposedLeader == self.getId()) ? ServerState.LEADING : learningState();</span><br><span class="line">    self.setPeerState(ss);</span><br><span class="line">    if (ss == ServerState.LEADING) &#123;</span><br><span class="line">        leadingVoteSet = voteSet;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>Above is all the core concepts and codes related to the election. In order to reduce the complexity, I omitted all the analisis of the code related to communication. I will post another blog if I can find time as writting in english will cost twice as much time as writting in chinese TT.</p>
]]></content>
  </entry>
  <entry>
    <title>What&#39;s virtual thread in JDK 19?</title>
    <url>/blog/2023/04/07/What-s-virtual-thread-in-JDK-19/</url>
    <content><![CDATA[<p>Recently, JDK 19 was released, which introduced several new features. One of the more notable features is the addition of virtual threads.</p>
<p>Many people may be confused about what virtual threads are and how they differ from the platform threads we currently use.</p>
<p>To understand virtual threads in JDK 19, we first need to understand how threads are implemented.</p>
<h2 id="The-Implementation-of-Threads"><a href="#The-Implementation-of-Threads" class="headerlink" title="The Implementation of Threads"></a>The Implementation of Threads</h2><p>We all know that in the operating system, threads are scheduling execution units that are more lightweight than processes. The introduction of threads can separate the allocation of resources and scheduling execution of a process, and each thread can share process resources as well as be independently scheduled.</p>
<p>In fact, there are three main ways of implementing threads: using kernel threads, using user threads, and using a combination of user threads and lightweight processes.</p>
<h2 id="Implementation-Using-Kernel-Threads"><a href="#Implementation-Using-Kernel-Threads" class="headerlink" title="Implementation Using Kernel Threads"></a>Implementation Using Kernel Threads</h2><p>Kernel-level threads (KLT) are threads supported directly by the operating system kernel. These threads are switched by the kernel, and the kernel schedules them by manipulating the scheduler. The kernel also maps thread tasks to various processors and provides API interfaces to manage threads to applications.</p>
<p>Applications generally do not directly use kernel threads, but instead use a high-level interface of kernel threads called Light Weight Processes (LWP). LWPs are threads as we usually understand them. Since each LWP is supported by a kernel thread, supporting kernel threads is a prerequisite for LWPs.</p>
<p>With kernel thread support, each LWP becomes an independent scheduling unit. Even if an LWP is blocked in a system call, it will not affect the entire process from continuing to work.</p>
<p>However, LWPs have their limitations. First of all, because they are implemented based on kernel threads, all thread operations, such as creation, destruction, and synchronization, require system calls. The cost of system calls is relatively high, requiring a switch between user mode and kernel mode. Secondly, each LWP requires support of a kernel thread. Therefore, LWPs consume certain kernel resources, such as kernel thread stack space, and the number of LWPs that a system can support is limited.</p>
<h2 id="Implementation-Using-User-Threads"><a href="#Implementation-Using-User-Threads" class="headerlink" title="Implementation Using User Threads"></a>Implementation Using User Threads</h2><p>In this implementation, a thread library is built in user space, and thread management is completed through a run-time system. Since this thread implementation is in user space, the operating system kernel is not aware of the existence of threads. Therefore, the kernel manages processes, and thread switching does not require kernel operations.</p>
<p>In this implementation, the relationship between a process and its threads is one-to-many.</p>
<p>The advantage of this thread implementation is that thread switching is fast, and it can run on any operating system as long as the thread library is implemented. However, the disadvantages are also significant. All thread operations need to be handled by the user program, and because most system calls are blocking, once a process is blocked, all threads in the process will be blocked as well. Additionally, mapping threads to other processors in a multi-processor system is also a significant problem.</p>
<h2 id="Mixed-Implementation-Using-User-Threads-and-Light-Weight-Processes"><a href="#Mixed-Implementation-Using-User-Threads-and-Light-Weight-Processes" class="headerlink" title="Mixed Implementation Using User Threads and Light Weight Processes"></a>Mixed Implementation Using User Threads and Light Weight Processes</h2><p>Another mixed implementation approach is to create threads in user space through a thread library, but thread scheduling is completed by the kernel. Multiple user threads can reuse multiple kernel threads through multiplexing. We won’t go into more detail on this approach.</p>
<h2 id="Implementation-of-Java-Threads"><a href="#Implementation-of-Java-Threads" class="headerlink" title="Implementation of Java Threads"></a>Implementation of Java Threads</h2><p>The above describes three implementation approaches for operating system threads. Different operating systems use different mechanisms to implement threads. For example, Windows uses kernel threads, while Solaris uses a mixed mode.</p>
<p>As a cross-platform programming language, Java’s thread implementation relies on specific operating systems. For commonly used operating systems such as Windows and Linux, kernel threads are used for implementation.</p>
<p>Therefore, when we create a thread in Java code, it actually needs to be mapped to the specific implementation of the operating system’s thread. The commonly used implementation through kernel threads requires kernel involvement during creation and scheduling, which is costly. Although Java provides a thread pool to avoid repeated thread creation, there is still a lot of room for optimization. Additionally, this implementation approach means that thread numbers are limited by machine resources.</p>
<h2 id="Virtual-Threads"><a href="#Virtual-Threads" class="headerlink" title="Virtual Threads"></a>Virtual Threads</h2><p>Virtual threads introduced in JDK 19 are lightweight threads implemented by JDK, which can avoid the additional cost of context switching. The implementation principle is that JDK no longer associates each thread one-to-one with an operating system thread. Instead, it maps multiple virtual threads to a small number of operating system threads and uses effective scheduling to avoid those context switches.</p>
<p>Furthermore, we can create a large number of virtual threads in the application without relying on the number of platform threads. These virtual threads are managed by the JVM, so they do not increase additional context switching overhead because they are stored as normal Java objects in RAM.</p>
<h2 id="Difference-between-Virtual-Threads-and-Platform-Threads"><a href="#Difference-between-Virtual-Threads-and-Platform-Threads" class="headerlink" title="Difference between Virtual Threads and Platform Threads"></a>Difference between Virtual Threads and Platform Threads</h2><p>Firstly, virtual threads are always daemon threads. The setDaemon(false) method cannot change a virtual thread to a non-daemon thread. Therefore, it is important to note that when all started non-daemon threads terminate, the JVM will terminate as well. This means that the JVM will not wait for virtual threads to complete before exiting.</p>
<p>Secondly, even if the setPriority() method is used, virtual threads always have normal priority and cannot change priority. Calling this method on a virtual thread has no effect.</p>
<p>In addition, virtual threads do not support methods such as stop(), suspend(), or resume(). Calling these methods on a virtual thread will result in an UnsupportedOperationException exception being thrown.</p>
<h2 id="How-to-Use-Virtual-Threads"><a href="#How-to-Use-Virtual-Threads" class="headerlink" title="How to Use Virtual Threads"></a>How to Use Virtual Threads</h2><p>Next, let’s introduce how to use virtual threads in JDK 19.</p>
<p>First, you can run a virtual thread using Thread.startVirtualThread():</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Thread.startVirtualThread(() -&gt; &#123;    System.out.println(&quot;Virtual thread is running...&quot;);&#125;); </span><br></pre></td></tr></table></figure>

<p>Secondly, you can also create a virtual thread using Thread.Builder. Thread provides ofPlatform() to create a platform thread and ofVirtual() to create a virtual thread.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Thread.Builder platformBuilder = Thread.ofPlatform().name(&quot;Platform Thread&quot;);Thread.Builder virtualBuilder = Thread.ofVirtual().name(&quot;Virtual Thread&quot;);</span><br><span class="line">Thread t1 = platformBuilder .start(() -&gt; &#123;...&#125;); Thread t2 = virtualBuilder.start(() -&gt; &#123;...&#125;);</span><br></pre></td></tr></table></figure>
<p>In addition, thread pools also support virtual threads, and you can create virtual threads using Executors.newVirtualThreadPerTaskExecutor():</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">try (var executor = Executors.newVirtualThreadPerTaskExecutor()) &#123;    IntStream.range(0, 10000).forEach(i -&gt; &#123;        executor.submit(() -&gt; &#123;            Thread.sleep(Duration.ofSeconds(1));            return i;        &#125;);    &#125;);&#125;</span><br></pre></td></tr></table></figure>
<p>However, it is not recommended to use virtual threads and thread pools together because the design of Java thread pools is to avoid the cost of creating new operating system threads, but creating virtual threads is not very costly, so there is no need to put them in the thread pool.</p>
<h2 id="Performance-Difference"><a href="#Performance-Difference" class="headerlink" title="Performance Difference"></a>Performance Difference</h2><p>After all this talk about virtual threads, can they really improve performance and how much? Let’s do a test.</p>
<p>Let’s write a simple task that waits for 1 second before printing a message to the console:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">final AtomicInteger atomicInteger = new AtomicInteger();</span><br><span class="line"></span><br><span class="line">Runnable runnable = () -&gt; &#123;</span><br><span class="line">  try &#123;</span><br><span class="line">    Thread.sleep(Duration.ofSeconds(1));</span><br><span class="line">  &#125; catch(Exception e) &#123;</span><br><span class="line">      System.out.println(e);</span><br><span class="line">  &#125;</span><br><span class="line">  System.out.println(&quot;Work Done - &quot; + atomicInteger.incrementAndGet());</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>Now, we will create 10,000 threads from this Runnable and use virtual threads and platform threads to execute them to compare their performance.</p>
<p>Let’s start with a familiar implementation of platform threads:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Instant start = Instant.now();</span><br><span class="line"></span><br><span class="line">try (var executor = Executors.newFixedThreadPool(100)) &#123;</span><br><span class="line">  for(int i = 0; i &lt; 10_000; i++) &#123;</span><br><span class="line">    executor.submit(runnable);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Instant finish = Instant.now();</span><br><span class="line">long timeElapsed = Duration.between(start, finish).toMillis();  </span><br><span class="line">System.out.println(&quot;Total time elapsed: &quot; + timeElapsed);</span><br></pre></td></tr></table></figure>
<p>The output is:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Total time elapsed: 100568</span><br></pre></td></tr></table></figure>
<p>About 100 seconds in total. Now let’s run it again using virtual threads.</p>
<p>Because in JDK 19, virtual threads are a preview API and are disabled by default. If you are using Intellij, and encounter this issue below:<br><img src="/blog/2023/04/07/What-s-virtual-thread-in-JDK-19/PreviewDisableIssue.png"></p>
<p>you need to add some configs to enable the preview features.</p>
<p><img src="/blog/2023/04/07/What-s-virtual-thread-in-JDK-19/previewsettings.png"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Instant start = Instant.now();</span><br><span class="line"></span><br><span class="line">try (var executor = Executors.newVirtualThreadPerTaskExecutor()) &#123;</span><br><span class="line">  for(int i = 0; i &lt; 10_000; i++) &#123;</span><br><span class="line">    executor.submit(runnable);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Instant finish = Instant.now();</span><br><span class="line">long timeElapsed = Duration.between(start, finish).toMillis();  </span><br><span class="line">System.out.println(&quot;Total time elapsed: &quot; + timeElapsed);</span><br></pre></td></tr></table></figure>
<p>We use Executors.newVirtualThreadPerTaskExecutor() to create virtual threads, and the output is:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Total time elapsed: 1219</span><br></pre></td></tr></table></figure>
<p>About 1.2 seconds in total.</p>
<p>The difference between 100 seconds and 1.2 seconds is enough to show the immediate improvement in virtual thread performance.</p>
]]></content>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/blog/2022/05/29/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>Spark Basic Tuning Guid</title>
    <url>/blog/2022/07/04/spark-tutorial/</url>
    <content><![CDATA[<h2 id="How-spark-works"><a href="#How-spark-works" class="headerlink" title="How spark works ?"></a>How spark works ?</h2><p>Nowadays python is becoming more and more popular among data scientists. Pyspark is rightfully become one of the most popular tools among them. Today let’s use a very simple Pyspark sample to help us understand more about spark.</p>
<h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p>First of all, spark architecture is based on the most typical distribute system design. </p>
<p><img src="/blog/2022/07/04/spark-tutorial/spark-basic-architecture.png"></p>
<p>But if we develop a pyspark script, how this pyspark script run on spark cluster?<br>Before we dive into the specific sample, let’s give a briefly introduction of the spark concepts.</p>
<h3 id="Spark-Concepts"><a href="#Spark-Concepts" class="headerlink" title="Spark Concepts"></a>Spark Concepts</h3><h4 id="stages"><a href="#stages" class="headerlink" title="stages"></a>stages</h4><p>Spark stages are the physical unit of execution for the computation of multiple tasks. The Spark stages are controlled by the Directed Acyclic Graph(DAG) for any data processing and transformations on the resilient distributed datasets(RDD). There are mainly two stages associated with the Spark frameworks such as, ShuffleMapStage and ResultStage. The Shuffle MapStage is the intermediate phase for the tasks which prepares data for subsequent stages, whereas resultStage is a final step to the spark function for the particular set of tasks in the spark job. ResultSet is associated with the initialization of parameter, counters and registry values in Spark.</p>
<h4 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h4><p>Driver is a Java process. This is the process where the main() method of our Scala, Java, Python program runs. It executes the user code and creates a SparkSession or SparkContext and the SparkSession is responsible to create DataFrame, DataSet, RDD, execute SQL, perform Transformation &amp; Action, etc.<br> At the end of the day, this is just a process on a physical machine that is responsible for maintaining the state of the application running on the cluster.</p>
<h4 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h4><p>Spark executors are the processes that perform the tasks assigned by the Spark driver. Executors have one core responsibility: take the tasks assigned by the driver, run them, and report back their state (success or failure) and results. Each Spark Application has its own separate executor processes.</p>
<h4 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h4><p>As we mentioned, tasks will be executed in executor. there might be many tasks running on couples of executors. tasks can map to the stages, each stage has couples of tasks. These tasks run the same code related to the stages.</p>
<h4 id="Job"><a href="#Job" class="headerlink" title="Job"></a>Job</h4><p>Data in Spark is abstracted as RDD, and it supports two types of operations: Transformation and Action. The code for Transformation operations will not be executed until an Action operation is encountered in our program.</p>
<p>Transformation operations mainly include: map, mapPartitions, flatMap, filter, union, groupByKey, repartition, cache, etc.</p>
<p>Action operations mainly include: reduce, collect, show, count, foreach, saveAsTextFile, etc.</p>
<p>When an Action operation is encountered in the program, a job will be submitted to execute the preceding operations. Therefore, it is important to note that if we declare that data needs to be cached or persisted but release it before an action operation, the data is actually not cached.</p>
<p>Usually, a task will have multiple jobs, and the jobs will be executed in a serial manner. After one job is completed, the next job will start.</p>
<h4 id="Stage"><a href="#Stage" class="headerlink" title="Stage"></a>Stage</h4><p>A job in Spark usually consists of one or more stages, which are executed in order. As mentioned earlier, a job may contain multiple operator operations that transform a parent RDD into a child RDD. During this process, there are two situations to consider: whether the data in the parent RDD enters different child RDDs. If the data in a parent RDD only goes to one child RDD, such as map, union, and other operations, it is called narrow dependency. Otherwise, it will form wide dependency, generally called shuffle dependency, such as groupByKey and other operations.</p>
<p>The partitioning of stages in a job is based on shuffle dependencies. Shuffle dependencies are the boundary between two stages in a job. Shuffle operations are generally the most time-consuming and resource-intensive part of a task. Because the data may be stored on different nodes in HDFS, the execution of the next stage first needs to fetch the data from the previous stage (shuffle read operation) and save it on its own node, which increases network communication and IO. Shuffle operations are actually a relatively complex process, which is not discussed here.</p>
<h2 id="Simple-Sample"><a href="#Simple-Sample" class="headerlink" title="Simple Sample"></a>Simple Sample</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1  from datetime import datetime</span><br><span class="line">2  now = datetime.now()</span><br><span class="line">3  current_time = now.strftime(&quot;%H:%M:%S&quot;)</span><br><span class="line">4  print(&quot;Current Time =&quot;, current_time)</span><br><span class="line">5  df = spark.read.format(&quot;csv&quot;).option(&quot;delimiter&quot;, &quot;,&quot;).load(&#x27;countries.csv&#x27;)</span><br><span class="line">6  df = df.withColumn(&#x27;C_DT&#x27;,lit(current_time))</span><br><span class="line">7  df.collect()</span><br><span class="line">8  print(&quot;new column added&quot;)</span><br></pre></td></tr></table></figure>
<p>This is a simple pyspark script. How it runs on a spark cluster?<br>In this case, Line 1, 2, 3, 4 will be executed on the driver. Line 5, 6 will be executed on the executor. Line 7, 8 will be executed on the driver.</p>
<h2 id="How-to-Select-Basic-Parameters"><a href="#How-to-Select-Basic-Parameters" class="headerlink" title="How to Select Basic Parameters"></a>How to Select Basic Parameters</h2><h3 id="Driver-Parameters"><a href="#Driver-Parameters" class="headerlink" title="Driver Parameters"></a>Driver Parameters</h3><h3 id="Executor-Basic-Parameters"><a href="#Executor-Basic-Parameters" class="headerlink" title="Executor Basic Parameters"></a>Executor Basic Parameters</h3><p>Apache Spark is a distributed computing system that is used to process large datasets. The tuning of Spark applications is important to optimize performance and avoid resource wastage. Below are some of the key parameters that can be optimized in Spark to improve performance:</p>
<ul>
<li><p>num-executors - This parameter specifies the number of executor processes to be used for a Spark job. Setting an appropriate value for this parameter is important as the default number of executors may not be sufficient for a Spark job. It is recommended to set 50-100 executor processes per Spark job.</p>
</li>
<li><p>executor-memory - This parameter sets the amount of memory to be allocated to each executor process. The memory allocated to each executor process can impact the performance of a Spark job. It is recommended to allocate 4-8GB memory to each executor process. However, this value can be adjusted based on the available resources in the cluster.</p>
</li>
<li><p>executor-cores - This parameter sets the number of CPU cores to be allocated to each executor process. A higher number of CPU cores can help the executor process execute tasks faster. It is recommended to allocate 2-4 CPU cores per executor process.</p>
</li>
<li><p>driver-memory - This parameter sets the amount of memory to be allocated to the driver process. The driver process is responsible for coordinating the execution of a Spark job. It is recommended to set 1GB memory for the driver process.</p>
</li>
<li><p>spark.default.parallelism - This parameter sets the default number of tasks to be executed in a stage of a Spark job. The default value of this parameter is set based on the number of partitions in the input data. It is recommended to set this parameter to 500-1000 tasks per stage.</p>
</li>
</ul>
<p>The tuning of these parameters should be based on the available resources in the cluster and the requirements of the Spark job. It is important to strike a balance between the number of executor processes, memory allocation, CPU core allocation, and the number of tasks per stage to achieve optimal performance.</p>
<p>Let’s say your data is 100GB, and its partition number is 10. You have 40 cores available in your spark cluster, and you want to run 10 tasks in parallel(), so how many executor should you set?<br>A good rule of thumb is to allocate at least 1GB of memory per core, and to reserve some memory for the overhead tasks. For example, if you have a cluster with 40 cores and 100GB of memory, you could allocate 4GB of memory per executor and reserve 1GB for overhead tasks, resulting in an effective memory size of 3GB per executor. So, in this case, you could set the spark.executor.memory parameter to 3g.<br>Again, you may need to experiment with different memory configurations and monitor the performance to find the optimal settings for your application. For instance, if one of your biggest dataset is more than 100GB, it has 10 partitions, and the data size of partitions is more than 10GB, then you need to consider to increase the executor memory to 12GB(10GB on-heap memory + 2GB overhead memory), and set the executor number to 10(consider there will be at most 10 tasks running in parallel)</p>
<h2 id="Spark-UI"><a href="#Spark-UI" class="headerlink" title="Spark UI"></a>Spark UI</h2><p>For Spark, the UI may seem like a simple component, but it plays a crucial role in our day-to-day troubleshooting. So I decided to discuss with you on some interesting points here.</p>
<h3 id="The-gap-between-different-tasks"><a href="#The-gap-between-different-tasks" class="headerlink" title="The gap between different tasks"></a>The gap between different tasks</h3><p><img src="/blog/2022/07/04/spark-tutorial/spark_event_timeline_1.jpg"></p>
<p>As you can see, there is a big gap between job 1 and job 2. But where the time went? What happend behind? When we jump into the coresponding tasks logs, we will find that the job was doing IO related operations. So Spark frame work will pend the next job starting until the current one finish the IO operations.</p>
<h3 id="Data-skewness"><a href="#Data-skewness" class="headerlink" title="Data skewness"></a>Data skewness</h3><p><img src="/blog/2022/07/04/spark-tutorial/data_skewness.png"></p>
<p>When data skew occurs, the vast majority of tasks execute very quickly, but a few tasks may execute extremely slowly. For example, if there are a total of 1000 tasks, 997 tasks may be completed within 1 minute, but the remaining two or three tasks may take one or two hours. This situation is very common.</p>
<p>An otherwise normal Spark job suddenly reports an OOM (out-of-memory) exception, and upon inspecting the exception stack, it is found to be caused by the business code we wrote. This situation is relatively rare.</p>
<p>Principle of Data Skew:</p>
<p>The principle of data skew is simple, when shuffling, the same keys on each node must be pulled to a task on a particular node for processing, such as aggregation or join operations based on keys. If a particular key has a very large amount of data, data skew occurs. For example, most keys correspond to 10 pieces of data, but a few keys correspond to 1 million pieces of data. As a result, most tasks may only be assigned 10 pieces of data and will be completed in 1 second, but a few tasks may be assigned 1 million pieces of data and take one or two hours to complete. Therefore, the overall progress of the Spark job is determined by the longest running task.</p>
<p>Therefore, when data skew occurs, the Spark job appears to run very slowly, and may even experience an out-of-memory error due to a task processing an excessive amount of data.</p>
<p>Once we know which stage the data skew occurs in, we need to use the stage partitioning principle to deduce which part of the code in the corresponding stage causes the skew. There must be a shuffle operator in this part of the code. To accurately deduce the correspondence between stage and code, a deep understanding of Spark’s source code is required. Here we can introduce a relatively simple and practical deducing method: whenever we see a shuffle operator in the Spark code or a SQL statement in Spark SQL that will cause a shuffle (such as a group by statement), we can determine that this point divides the code into two stages.</p>
]]></content>
  </entry>
  <entry>
    <title>spark executor memory introduction</title>
    <url>/blog/2023/03/19/spark-executor-memory-introduction/</url>
    <content><![CDATA[<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>Hello everyone, today I would like to introduce a problem that we often encounter in our work: “Container killed by YARN for exceeding memory limits”. What causes this problem? What is the difference between this problem and OOM? What is the relationship between this problem and the memory structure of Spark Executor? Today, let’s explore these three questions.</p>
<p>First of all, a Spark cluster will start two types of JVM processes, the Driver and the Executor. The former is the main control process, responsible for creating the Spark context, submitting Spark jobs, and transforming jobs into compute tasks. It also coordinates task scheduling among the various Executor processes. The latter is responsible for executing specific compute tasks on worker nodes, returning results to the Driver, and providing storage functionality for RDDs that need to be persisted. Since the memory management of the Driver is relatively simple, this article mainly analyzes the memory management of the Executor. Hereafter, the term “Spark memory” refers specifically to the memory of the Executor. [Note that this article targets Spark version 2.x, deployed in YARN mode.]</p>
<p><em>the memory layout of the entire Executor side is shown in the following figure.</em></p>
<p><img src="/blog/2023/03/19/spark-executor-memory-introduction/executorMemoryOverview.jpg"></p>
<p>We can see that in the Yarn cluster management mode, Spark runs in the form of Executor Containers in NodeManager, with its maximum available memory limit specified by yarn.scheduler.maximum-allocation-mb, which we call MonitorMemory.</p>
<h2 id="Executor-Memory-Strucutre-On-Yarn-Overview"><a href="#Executor-Memory-Strucutre-On-Yarn-Overview" class="headerlink" title="Executor Memory Strucutre On Yarn Overview"></a>Executor Memory Strucutre On Yarn Overview</h2><p>The Executor memory area on yarn is divided into two parts:</p>
<ol>
<li>JVM off-heap memory<br>The size is specified by the spark.yarn.executor.memoryOverhead parameter, with a default size of executorMemory * 0.10, with a minimum of 384m. This memory is mainly used for JVM itself, strings, NIO Buffer (Direct Buffer), and other overheads. This part is for user code and Spark’s non-operable memory, and can be adjusted by tuning parameters when it is insufficient.</li>
<li>On-heap memory (Spark Executor Memory)<br>The size is configured by the –executor-memory or spark.executor.memory parameter when the Spark application starts, which is the maximum heap memory allocated by the JVM (-Xmx). To more efficiently use this part of memory, Spark has logically partitioned and managed it. We will explain the unified memory management in detail below.</li>
</ol>
<p>For Yarn clusters, there is a limit: ExecutorMemory + MemoryOverhead &lt;&#x3D; monitormemory. If the sum of ExecutorMemory and MemoryOverhead specified when the application is submitted is greater than MonitorMemory, the Executor will fail to be allocated. If the actual memory usage exceeds the upper limit threshold during runtime, the Yarn will terminate (kill) the Executor process.<br>After Spark 1.6, unified memory management was introduced, including two areas: on-heap memory and off-heap memory. The following is a detailed explanation of these two areas.<br>By default, Spark only uses on-heap memory. Spark manages on-heap memory in a logical “planning” manner. The on-heap memory area on the Executor side is logically divided into the following four areas:</p>
<h3 id="Spark-Executor-Memory"><a href="#Spark-Executor-Memory" class="headerlink" title="Spark Executor Memory"></a>Spark Executor Memory</h3><h4 id="Execution-Memory"><a href="#Execution-Memory" class="headerlink" title="Execution Memory"></a>Execution Memory</h4><p>mainly used to store temporary data during calculations such as Shuffle, Join, Sort, and Aggregation.<br>Storage Memory: mainly used to store Spark cache data such as RDD caching, unroll data, and broadcast data.<br>User Memory: mainly used to store data needed for RDD transformation operations, such as RDD dependencies.<br>Reserved Memory: system reserved memory, which is used to store Spark internal objects [300MB].</p>
<h4 id="Reserved-Memory"><a href="#Reserved-Memory" class="headerlink" title="Reserved Memory"></a>Reserved Memory</h4><p>The system reserves memory for storing Spark internal objects. Its size is hard-coded in the code and its value equals to 300MB, which cannot be modified (if in a testing environment, we can modify it using the spark.testing.reservedMemory parameter); if the memory allocated to an Executor is less than 1.5 * 300 &#x3D; 450M, the Executor will not be able to run.</p>
<h4 id="Storage-Memory"><a href="#Storage-Memory" class="headerlink" title="Storage Memory"></a>Storage Memory</h4><p>Mainly used to store Spark cache data, such as RDD caching, broadcast data, and unroll data. The memory occupancy ratio is UsableMemory * spark.memory.fraction * spark.memory.storageFraction. In Spark 2+, by default, Storage Memory and Execution Memory each account for approximately 30% of the system’s total memory (1 * 0.6 * 0.5 &#x3D; 0.3). In Unified Memory management, these two types of memory can be borrowed from each other. We will discuss the specific borrowing mechanism in the next section.</p>
<h4 id="Execution-Memory-1"><a href="#Execution-Memory-1" class="headerlink" title="Execution Memory"></a>Execution Memory</h4><p>Mainly used to store temporary data during calculations such as Shuffle, Join, Sort, and Aggregation. The memory occupancy ratio is UsableMemory * spark.memory.fraction * (1 - spark.memory.storageFraction). In Spark 2+, by default, Storage Memory and Execution Memory each account for approximately 30% of the system’s total memory (1 * 0.6 * (1 - 0.5) &#x3D; 0.3). In Unified Memory management, these two types of memory can be borrowed from each other.</p>
<h4 id="Other-x2F-User-Memory"><a href="#Other-x2F-User-Memory" class="headerlink" title="Other&#x2F;User Memory"></a>Other&#x2F;User Memory</h4><p>mainly used to store data needed for RDD transformation operations, such as RDD dependencies. The memory occupancy ratio is UsableMemory * (1 - spark.memory.fraction). In Spark 2+, it defaults to 40% of the available memory (1 * (1 - 0.6) &#x3D; 0.4).</p>
<p>There are a few interesting points here that we can elaborate on:</p>
<ol>
<li>Why is 300MB reserved memory set?<br>In the initial version of Unified Memory Management, the “Other” part of memory did not have a fixed value of 300MB, but instead used a percentage similar to static memory management, initially set to 25%. However, in practice, setting a low amount of memory (e.g. 1GB) led to OOM errors. This issue is discussed in detail here: “Make unified memory management work with small heaps”. Therefore, the “Other” part of memory was modified to reserve 300MB of memory upfront.</li>
<li>spark.memory.fraction decreased from 0.75 to 0.6<br>The initial value of spark.memory.fraction was set to 0.75, and many analyses of Unified Memory Management also described it as such. However, it was found that this value was set too high and resulted in long GC times. Spark 2.0 lowered the default value to 0.6. For more detailed discussion, see “Reduce spark.memory.fraction default to avoid overrunning old gen in JVM default config”.</li>
<li>Off-heap Memory<br>Spark 1.6 introduced Off-heap memory (see SPARK-11389). In this mode, memory is not allocated within the JVM, but instead uses Java’s unsafe API to directly request memory from the operating system, similar to C’s malloc(). This allows Spark to directly access off-heap memory, reducing unnecessary memory overhead and frequent GC scans and collections, improving processing performance. Additionally, off-heap memory can be accurately allocated and released, and the space occupied by serialized data can be accurately calculated, making it easier to manage and reducing errors compared to on-heap memory. The downside is that one must write the logic for memory allocation and release themselves.</li>
</ol>
<h2 id="Task-Memory-Manager"><a href="#Task-Memory-Manager" class="headerlink" title="Task Memory Manager"></a>Task Memory Manager</h2><p>Tasks in the Executor are executed as threads and share the JVM’s resources (i.e., Execution memory). There is no strong isolation between the memory resources of tasks (i.e., tasks do not have a dedicated heap area). Therefore, it is possible for an earlier arriving task to occupy a large amount of memory, causing a later arriving task to be suspended due to insufficient memory.</p>
<p>In Spark’s task memory management, a HashMap is used to store the mapping between tasks and their memory consumption. The amount of memory that each task can occupy is half to one over n of the potential available computing memory (potential available computing memory is the initial computing memory plus preemptive storage memory). When the remaining memory is less than half to one over n, the task will be suspended until other tasks release execution memory, and the memory lower limit of half to one over n is satisfied, and the task is awakened. Here, n is the number of active tasks in the current Executor.</p>
<p>For example, if the Execution memory size is 10GB and there are 5 running tasks in the current Executor, the range of memory that this task can apply for is from 10 &#x2F; (2 * 5) to 10 &#x2F; 5, which is 1GB to 2GB.</p>
<p>During task execution, if more memory is needed, it will be requested. If there is available memory, the request will be automatically successful; otherwise, an OutOfMemoryError will be thrown. The maximum number of tasks that can run simultaneously in each Executor is determined by the number of CPU cores N allocated by the Executor and the number of CPU cores C required by each task. Specifically:</p>
<p>N &#x3D; spark.executor.cores<br>C &#x3D; spark.task.cpus</p>
<p>Therefore, the maximum task parallelism of each Executor can be represented as TP &#x3D; N &#x2F; C. The value of C depends on the application type, and most applications can use the default value of 1. Thus, the main factor affecting the maximum task parallelism (i.e., the maximum number of active tasks) in the Executor is N.</p>
<p>Based on the memory usage characteristics of tasks, the Executor memory model described above can be simply abstracted as the following diagram:</p>
<p><img src="/blog/2023/03/19/spark-executor-memory-introduction/unifiedMemory.jpg"></p>
<h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>To better understand the use of on-heap and off-heap memory mentioned above, here is a simple example.<br>3.1. Only On-Heap Memory Used<br>We have submitted a Spark job with the following memory configuration:<br><code>--executor-memory 18g.</code></p>
<p>As we have not set the <code>spark.memory.fraction</code> and <code>spark.memory.storageFraction</code> parameters, we can see from the Spark UI that the available Storage Memory is displayed as follows:</p>
<p><img src="/blog/2023/03/19/spark-executor-memory-introduction/sparkHistoryUi.jpg"></p>
<p>From the image above, we can see that the available Storage Memory is 10.1GB. How was this number obtained? According to the previous rule, we can perform the following calculations:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemMemory = spark.executor.memory</span><br><span class="line">reservedMemory = 300MB</span><br><span class="line">usableMemory = systemMemory - reservedMemory</span><br><span class="line">StorageMemory= usableMemory * spark.memory.fraction * spark.memory.storageFraction</span><br></pre></td></tr></table></figure>

<p>If we substitute the values, we get the following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemMemory = 18Gb = 19327352832 bytes</span><br><span class="line">reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800</span><br><span class="line">usableMemory = systemMemory - reservedMemory = 19327352832 - 314572800 = 19012780032</span><br><span class="line">StorageMemory = usableMemory * spark.memory.fraction * spark.memory.storageFraction = 19012780032 * 0.6 * 0.5 = 5703834009.6 = 5.312109375GB</span><br></pre></td></tr></table></figure>
<p>The value of 10.1GB displayed on the Spark UI does not match the value we obtained. Why is that? This is because the available Storage Memory displayed on the Spark UI is actually the sum of the Execution Memory and Storage Memory, i.e., usableMemory * spark.memory.fraction:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">StorageMemory = usableMemory * spark.memory.fraction = 19012780032 * 0.6 = 11407668019.2 = 10.62421GB</span><br></pre></td></tr></table></figure>
<p>This value is also incorrect because even though we have set –executor-memory 18g, the memory available to Spark’s Executor is not as large as this. It is only 17179869184 bytes, which is obtained from Runtime.getRuntime.maxMemory. Therefore, we can perform the following calculations:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemMemory = 17179869184 bytes</span><br><span class="line">reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800</span><br><span class="line">usableMemory = systemMemory - reservedMemory = 17179869184 - 314572800 = 16865296384</span><br><span class="line">StorageMemory= usableMemory * spark.memory.fraction = 16865296384 * 0.6 = 9.42421875 GB</span><br></pre></td></tr></table></figure>
<p>When we convert 16865296384 * 0.6 bytes to GB by dividing it by 1024 * 1024 * 1024, we get 9.42421875 GB, which is still different from the value displayed on the UI. This is because the Spark UI converts bytes to GB by dividing them by 1000 * 1000 * 1000, as shown below:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemMemory = 17179869184 bytes</span><br><span class="line">reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800</span><br><span class="line">usableMemory = systemMemory - reservedMemory = 17179869184 - 314572800 = 16865296384</span><br><span class="line">StorageMemory = usableMemory * spark.memory.fraction = 16865296384 * 0.6 bytes = 16865296384 * 0.6 / (1000 * 1000 * 1000) = 10.1GB</span><br></pre></td></tr></table></figure>
<p>Now, the value matches. </p>
<p>We have set –executor-memory to 18g, but the memory that Spark’s executor side can get through Runtime.getRuntime.maxMemory is actually not that big, it’s only 17179869184 bytes. How is this data calculated?</p>
<p>Runtime.getRuntime.maxMemory is the maximum memory that the program can use, and its value is smaller than the actual configured executor memory value. This is because the heap portion of the memory allocation pool is divided into three parts: Eden, Survivor, and Tenured. There are two Survivor areas in these three parts, and we can only use one of them at any time. Therefore, we can use the following formula to describe it:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. ExecutorMemory = Eden + 2 * Survivor + Tenured</span><br><span class="line">2. Runtime.getRuntime.maxMemory = Eden + Survivor + Tenured</span><br></pre></td></tr></table></figure>
<p>The value of 17179869184 bytes above may be different depending on your GC configuration, but the calculation formula above is the same.</p>
<h3 id="Using-Heap-and-Off-Heap-Memory"><a href="#Using-Heap-and-Off-Heap-Memory" class="headerlink" title="Using Heap and Off-Heap Memory"></a>Using Heap and Off-Heap Memory</h3><p>Now, what if we enable off-heap memory? Our memory configurations are as follows:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">spark.executor.memory 18g</span><br><span class="line">spark.memory.offHeap.enabled true</span><br><span class="line">spark.memory.offHeap.size 10737418240</span><br></pre></td></tr></table></figure>
<p>From the above, we can see that the off-heap memory is 10GB. Now, the available Storage Memory shown on Spark UI is 20.9GB, as follows:</p>
<p><img src="/blog/2023/03/19/spark-executor-memory-introduction/sparkMemoryOffheapMemory.jpg"></p>
<p>Actually, the available Storage Memory shown on Spark UI is the sum of heap memory and off-heap memory. The calculation formula is as follows:</p>
<ul>
<li><p>Heap</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemMemory = 17179869184 bytes</span><br><span class="line">reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800</span><br><span class="line">usableMemory = systemMemory - reservedMemory = 17179869184 - 314572800 = 16865296384</span><br><span class="line">totalOnHeapStorageMemory = usableMemory * spark.memory.fraction = 16865296384 * 0.6 = 10119177830</span><br></pre></td></tr></table></figure>
</li>
<li><p>Off-Heap</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">totalOffHeapStorageMemory = spark.memory.offHeap.size = 10737418240</span><br></pre></td></tr></table></figure></li>
<li><p>Total Storage Memory</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">StorageMemory = totalOnHeapStorageMemory + totalOffHeapStorageMemory = (10119177830 + 10737418240) bytes = (20856596070 / (1000 * 1000 * 1000)) GB = 20.9 GB</span><br></pre></td></tr></table></figure></li>
</ul>
<p>By now, we have shown you a clear picture of the memory architecture and management logic of Spark executor. Going back to the first two questions we mentioned earlier, when the executor serializes data, all memory management is actually not handled by the JVM. Therefore, continuous requests can lead to the YARN container exceeding the memory limit and being killed by YARN. And when Spark encounters OOM due to data skew issues, the OOM actually occurs inside the JVM. As a user, we can easily determine the direct cause of our problem based on the different error messages of these two OOMs, whether it is caused by the overhead memory area or the insufficient no-heap JVM memory, and thus correctly deduce the direction we need to optimize.</p>
]]></content>
  </entry>
</search>
