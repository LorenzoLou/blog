<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/blog/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/blog/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/blog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/blog/css/main.css">


<link rel="stylesheet" href="/blog/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"lorenzolou.github.io","root":"/blog/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="How spark works ?Nowadays python is becoming more and more popular among data scientists. Pyspark is rightfully become one of the most popular tools among them. Today let’s use a very simple Pyspark s">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark Basic Tuning Guid">
<meta property="og:url" content="https://lorenzolou.github.io/blog/2022/07/04/spark-tutorial/index.html">
<meta property="og:site_name" content="Lorenzo&#39;s Playground">
<meta property="og:description" content="How spark works ?Nowadays python is becoming more and more popular among data scientists. Pyspark is rightfully become one of the most popular tools among them. Today let’s use a very simple Pyspark s">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://lorenzolou.github.io/blog/2022/07/04/spark-tutorial/spark-basic-architecture.png">
<meta property="og:image" content="https://lorenzolou.github.io/blog/2022/07/04/spark-tutorial/spark_event_timeline_1.jpg">
<meta property="og:image" content="https://lorenzolou.github.io/blog/2022/07/04/spark-tutorial/data_skewness.png">
<meta property="article:published_time" content="2022-07-03T23:39:23.000Z">
<meta property="article:modified_time" content="2023-03-25T03:54:10.000Z">
<meta property="article:author" content="Lorenzo Lou">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lorenzolou.github.io/blog/2022/07/04/spark-tutorial/spark-basic-architecture.png">

<link rel="canonical" href="https://lorenzolou.github.io/blog/2022/07/04/spark-tutorial/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Spark Basic Tuning Guid | Lorenzo's Playground</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/blog/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Lorenzo's Playground</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/blog/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/blog/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://lorenzolou.github.io/blog/2022/07/04/spark-tutorial/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="Lorenzo Lou">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lorenzo's Playground">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Spark Basic Tuning Guid
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-07-04 07:39:23" itemprop="dateCreated datePublished" datetime="2022-07-04T07:39:23+08:00">2022-07-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-03-25 11:54:10" itemprop="dateModified" datetime="2023-03-25T11:54:10+08:00">2023-03-25</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="How-spark-works"><a href="#How-spark-works" class="headerlink" title="How spark works ?"></a>How spark works ?</h2><p>Nowadays python is becoming more and more popular among data scientists. Pyspark is rightfully become one of the most popular tools among them. Today let’s use a very simple Pyspark sample to help us understand more about spark.</p>
<h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p>First of all, spark architecture is based on the most typical distribute system design. </p>
<p><img src="/blog/2022/07/04/spark-tutorial/spark-basic-architecture.png"></p>
<p>But if we develop a pyspark script, how this pyspark script run on spark cluster?<br>Before we dive into the specific sample, let’s give a briefly introduction of the spark concepts.</p>
<h3 id="Spark-Concepts"><a href="#Spark-Concepts" class="headerlink" title="Spark Concepts"></a>Spark Concepts</h3><h4 id="stages"><a href="#stages" class="headerlink" title="stages"></a>stages</h4><p>Spark stages are the physical unit of execution for the computation of multiple tasks. The Spark stages are controlled by the Directed Acyclic Graph(DAG) for any data processing and transformations on the resilient distributed datasets(RDD). There are mainly two stages associated with the Spark frameworks such as, ShuffleMapStage and ResultStage. The Shuffle MapStage is the intermediate phase for the tasks which prepares data for subsequent stages, whereas resultStage is a final step to the spark function for the particular set of tasks in the spark job. ResultSet is associated with the initialization of parameter, counters and registry values in Spark.</p>
<h4 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h4><p>Driver is a Java process. This is the process where the main() method of our Scala, Java, Python program runs. It executes the user code and creates a SparkSession or SparkContext and the SparkSession is responsible to create DataFrame, DataSet, RDD, execute SQL, perform Transformation &amp; Action, etc.<br> At the end of the day, this is just a process on a physical machine that is responsible for maintaining the state of the application running on the cluster.</p>
<h4 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h4><p>Spark executors are the processes that perform the tasks assigned by the Spark driver. Executors have one core responsibility: take the tasks assigned by the driver, run them, and report back their state (success or failure) and results. Each Spark Application has its own separate executor processes.</p>
<h4 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h4><p>As we mentioned, tasks will be executed in executor. there might be many tasks running on couples of executors. tasks can map to the stages, each stage has couples of tasks. These tasks run the same code related to the stages.</p>
<h4 id="Job"><a href="#Job" class="headerlink" title="Job"></a>Job</h4><p>Data in Spark is abstracted as RDD, and it supports two types of operations: Transformation and Action. The code for Transformation operations will not be executed until an Action operation is encountered in our program.</p>
<p>Transformation operations mainly include: map, mapPartitions, flatMap, filter, union, groupByKey, repartition, cache, etc.</p>
<p>Action operations mainly include: reduce, collect, show, count, foreach, saveAsTextFile, etc.</p>
<p>When an Action operation is encountered in the program, a job will be submitted to execute the preceding operations. Therefore, it is important to note that if we declare that data needs to be cached or persisted but release it before an action operation, the data is actually not cached.</p>
<p>Usually, a task will have multiple jobs, and the jobs will be executed in a serial manner. After one job is completed, the next job will start.</p>
<h4 id="Stage"><a href="#Stage" class="headerlink" title="Stage"></a>Stage</h4><p>A job in Spark usually consists of one or more stages, which are executed in order. As mentioned earlier, a job may contain multiple operator operations that transform a parent RDD into a child RDD. During this process, there are two situations to consider: whether the data in the parent RDD enters different child RDDs. If the data in a parent RDD only goes to one child RDD, such as map, union, and other operations, it is called narrow dependency. Otherwise, it will form wide dependency, generally called shuffle dependency, such as groupByKey and other operations.</p>
<p>The partitioning of stages in a job is based on shuffle dependencies. Shuffle dependencies are the boundary between two stages in a job. Shuffle operations are generally the most time-consuming and resource-intensive part of a task. Because the data may be stored on different nodes in HDFS, the execution of the next stage first needs to fetch the data from the previous stage (shuffle read operation) and save it on its own node, which increases network communication and IO. Shuffle operations are actually a relatively complex process, which is not discussed here.</p>
<h2 id="Simple-Sample"><a href="#Simple-Sample" class="headerlink" title="Simple Sample"></a>Simple Sample</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1  from datetime import datetime</span><br><span class="line">2  now = datetime.now()</span><br><span class="line">3  current_time = now.strftime(&quot;%H:%M:%S&quot;)</span><br><span class="line">4  print(&quot;Current Time =&quot;, current_time)</span><br><span class="line">5  df = spark.read.format(&quot;csv&quot;).option(&quot;delimiter&quot;, &quot;,&quot;).load(&#x27;countries.csv&#x27;)</span><br><span class="line">6  df = df.withColumn(&#x27;C_DT&#x27;,lit(current_time))</span><br><span class="line">7  df.collect()</span><br><span class="line">8  print(&quot;new column added&quot;)</span><br></pre></td></tr></table></figure>
<p>This is a simple pyspark script. How it runs on a spark cluster?<br>In this case, Line 1, 2, 3, 4 will be executed on the driver. Line 5, 6 will be executed on the executor. Line 7, 8 will be executed on the driver.</p>
<h2 id="How-to-Select-Basic-Parameters"><a href="#How-to-Select-Basic-Parameters" class="headerlink" title="How to Select Basic Parameters"></a>How to Select Basic Parameters</h2><h3 id="Driver-Parameters"><a href="#Driver-Parameters" class="headerlink" title="Driver Parameters"></a>Driver Parameters</h3><h3 id="Executor-Basic-Parameters"><a href="#Executor-Basic-Parameters" class="headerlink" title="Executor Basic Parameters"></a>Executor Basic Parameters</h3><p>Apache Spark is a distributed computing system that is used to process large datasets. The tuning of Spark applications is important to optimize performance and avoid resource wastage. Below are some of the key parameters that can be optimized in Spark to improve performance:</p>
<ul>
<li><p>num-executors - This parameter specifies the number of executor processes to be used for a Spark job. Setting an appropriate value for this parameter is important as the default number of executors may not be sufficient for a Spark job. It is recommended to set 50-100 executor processes per Spark job.</p>
</li>
<li><p>executor-memory - This parameter sets the amount of memory to be allocated to each executor process. The memory allocated to each executor process can impact the performance of a Spark job. It is recommended to allocate 4-8GB memory to each executor process. However, this value can be adjusted based on the available resources in the cluster.</p>
</li>
<li><p>executor-cores - This parameter sets the number of CPU cores to be allocated to each executor process. A higher number of CPU cores can help the executor process execute tasks faster. It is recommended to allocate 2-4 CPU cores per executor process.</p>
</li>
<li><p>driver-memory - This parameter sets the amount of memory to be allocated to the driver process. The driver process is responsible for coordinating the execution of a Spark job. It is recommended to set 1GB memory for the driver process.</p>
</li>
<li><p>spark.default.parallelism - This parameter sets the default number of tasks to be executed in a stage of a Spark job. The default value of this parameter is set based on the number of partitions in the input data. It is recommended to set this parameter to 500-1000 tasks per stage.</p>
</li>
</ul>
<p>The tuning of these parameters should be based on the available resources in the cluster and the requirements of the Spark job. It is important to strike a balance between the number of executor processes, memory allocation, CPU core allocation, and the number of tasks per stage to achieve optimal performance.</p>
<p>Let’s say your data is 100GB, and its partition number is 10. You have 40 cores available in your spark cluster, and you want to run 10 tasks in parallel(), so how many executor should you set?<br>A good rule of thumb is to allocate at least 1GB of memory per core, and to reserve some memory for the overhead tasks. For example, if you have a cluster with 40 cores and 100GB of memory, you could allocate 4GB of memory per executor and reserve 1GB for overhead tasks, resulting in an effective memory size of 3GB per executor. So, in this case, you could set the spark.executor.memory parameter to 3g.<br>Again, you may need to experiment with different memory configurations and monitor the performance to find the optimal settings for your application. For instance, if one of your biggest dataset is more than 100GB, it has 10 partitions, and the data size of partitions is more than 10GB, then you need to consider to increase the executor memory to 12GB(10GB on-heap memory + 2GB overhead memory), and set the executor number to 10(consider there will be at most 10 tasks running in parallel)</p>
<h2 id="Spark-UI"><a href="#Spark-UI" class="headerlink" title="Spark UI"></a>Spark UI</h2><p>For Spark, the UI may seem like a simple component, but it plays a crucial role in our day-to-day troubleshooting. So I decided to discuss with you on some interesting points here.</p>
<h3 id="The-gap-between-different-tasks"><a href="#The-gap-between-different-tasks" class="headerlink" title="The gap between different tasks"></a>The gap between different tasks</h3><p><img src="/blog/2022/07/04/spark-tutorial/spark_event_timeline_1.jpg"></p>
<p>As you can see, there is a big gap between job 1 and job 2. But where the time went? What happend behind? When we jump into the coresponding tasks logs, we will find that the job was doing IO related operations. So Spark frame work will pend the next job starting until the current one finish the IO operations.</p>
<h3 id="Data-skewness"><a href="#Data-skewness" class="headerlink" title="Data skewness"></a>Data skewness</h3><p><img src="/blog/2022/07/04/spark-tutorial/data_skewness.png"></p>
<p>When data skew occurs, the vast majority of tasks execute very quickly, but a few tasks may execute extremely slowly. For example, if there are a total of 1000 tasks, 997 tasks may be completed within 1 minute, but the remaining two or three tasks may take one or two hours. This situation is very common.</p>
<p>An otherwise normal Spark job suddenly reports an OOM (out-of-memory) exception, and upon inspecting the exception stack, it is found to be caused by the business code we wrote. This situation is relatively rare.</p>
<p>Principle of Data Skew:</p>
<p>The principle of data skew is simple, when shuffling, the same keys on each node must be pulled to a task on a particular node for processing, such as aggregation or join operations based on keys. If a particular key has a very large amount of data, data skew occurs. For example, most keys correspond to 10 pieces of data, but a few keys correspond to 1 million pieces of data. As a result, most tasks may only be assigned 10 pieces of data and will be completed in 1 second, but a few tasks may be assigned 1 million pieces of data and take one or two hours to complete. Therefore, the overall progress of the Spark job is determined by the longest running task.</p>
<p>Therefore, when data skew occurs, the Spark job appears to run very slowly, and may even experience an out-of-memory error due to a task processing an excessive amount of data.</p>
<p>Once we know which stage the data skew occurs in, we need to use the stage partitioning principle to deduce which part of the code in the corresponding stage causes the skew. There must be a shuffle operator in this part of the code. To accurately deduce the correspondence between stage and code, a deep understanding of Spark’s source code is required. Here we can introduce a relatively simple and practical deducing method: whenever we see a shuffle operator in the Spark code or a SQL statement in Spark SQL that will cause a shuffle (such as a group by statement), we can determine that this point divides the code into two stages.</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/blog/2022/06/19/Zookeeper-Election/" rel="prev" title="Zookeeper Election Mechanism Introduction">
      <i class="fa fa-chevron-left"></i> Zookeeper Election Mechanism Introduction
    </a></div>
      <div class="post-nav-item">
    <a href="/blog/2023/03/19/spark-executor-memory-introduction/" rel="next" title="spark executor memory introduction">
      spark executor memory introduction <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#How-spark-works"><span class="nav-number">1.</span> <span class="nav-text">How spark works ?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Architecture"><span class="nav-number">2.</span> <span class="nav-text">Architecture</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-Concepts"><span class="nav-number">2.1.</span> <span class="nav-text">Spark Concepts</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#stages"><span class="nav-number">2.1.1.</span> <span class="nav-text">stages</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Driver"><span class="nav-number">2.1.2.</span> <span class="nav-text">Driver</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Executor"><span class="nav-number">2.1.3.</span> <span class="nav-text">Executor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Task"><span class="nav-number">2.1.4.</span> <span class="nav-text">Task</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Job"><span class="nav-number">2.1.5.</span> <span class="nav-text">Job</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Stage"><span class="nav-number">2.1.6.</span> <span class="nav-text">Stage</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Simple-Sample"><span class="nav-number">3.</span> <span class="nav-text">Simple Sample</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#How-to-Select-Basic-Parameters"><span class="nav-number">4.</span> <span class="nav-text">How to Select Basic Parameters</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Driver-Parameters"><span class="nav-number">4.1.</span> <span class="nav-text">Driver Parameters</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Executor-Basic-Parameters"><span class="nav-number">4.2.</span> <span class="nav-text">Executor Basic Parameters</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-UI"><span class="nav-number">5.</span> <span class="nav-text">Spark UI</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#The-gap-between-different-tasks"><span class="nav-number">5.1.</span> <span class="nav-text">The gap between different tasks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Data-skewness"><span class="nav-number">5.2.</span> <span class="nav-text">Data skewness</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Lorenzo Lou</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/blog/archives/">
        
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lorenzo Lou</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/blog/lib/anime.min.js"></script>
  <script src="/blog/lib/velocity/velocity.min.js"></script>
  <script src="/blog/lib/velocity/velocity.ui.min.js"></script>

<script src="/blog/js/utils.js"></script>

<script src="/blog/js/motion.js"></script>


<script src="/blog/js/schemes/pisces.js"></script>


<script src="/blog/js/next-boot.js"></script>




  




  
<script src="/blog/js/local-search.js"></script>













  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '686954989cace0b7e2ec',
      clientSecret: 'd06f0e8553b92cf39b567431dbeaeec90c9f59a8',
      repo        : 'blog',
      owner       : 'LorenzoLou',
      admin       : ['LorenzoLou'],
      id          : '4ae1eccec00f34cd49c9a6291f9bff7b',
        language: 'en',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
