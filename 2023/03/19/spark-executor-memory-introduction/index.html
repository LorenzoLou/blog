<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/blog/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/blog/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/blog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/blog/css/main.css">


<link rel="stylesheet" href="/blog/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"lorenzolou.github.io","root":"/blog/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="OverviewHello everyone, today I would like to introduce a problem that we often encounter in our work: “Container killed by YARN for exceeding memory limits”. What causes this problem? What is the dif">
<meta property="og:type" content="article">
<meta property="og:title" content="spark executor memory introduction">
<meta property="og:url" content="https://lorenzolou.github.io/blog/2023/03/19/spark-executor-memory-introduction/index.html">
<meta property="og:site_name" content="Lorenzo&#39;s Playground">
<meta property="og:description" content="OverviewHello everyone, today I would like to introduce a problem that we often encounter in our work: “Container killed by YARN for exceeding memory limits”. What causes this problem? What is the dif">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://lorenzolou.github.io/blog/2023/03/19/spark-executor-memory-introduction/executorMemoryOverview.jpg">
<meta property="og:image" content="https://lorenzolou.github.io/blog/2023/03/19/spark-executor-memory-introduction/unifiedMemory.jpg">
<meta property="og:image" content="https://lorenzolou.github.io/blog/2023/03/19/spark-executor-memory-introduction/sparkHistoryUi.jpg">
<meta property="og:image" content="https://lorenzolou.github.io/blog/2023/03/19/spark-executor-memory-introduction/sparkMemoryOffheapMemory.jpg">
<meta property="article:published_time" content="2023-03-19T07:56:48.000Z">
<meta property="article:modified_time" content="2023-03-25T03:55:20.226Z">
<meta property="article:author" content="Lorenzo Lou">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lorenzolou.github.io/blog/2023/03/19/spark-executor-memory-introduction/executorMemoryOverview.jpg">

<link rel="canonical" href="https://lorenzolou.github.io/blog/2023/03/19/spark-executor-memory-introduction/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>spark executor memory introduction | Lorenzo's Playground</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/blog/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Lorenzo's Playground</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/blog/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/blog/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://lorenzolou.github.io/blog/2023/03/19/spark-executor-memory-introduction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="Lorenzo Lou">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lorenzo's Playground">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          spark executor memory introduction
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-03-19 15:56:48" itemprop="dateCreated datePublished" datetime="2023-03-19T15:56:48+08:00">2023-03-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-03-25 11:55:20" itemprop="dateModified" datetime="2023-03-25T11:55:20+08:00">2023-03-25</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>Hello everyone, today I would like to introduce a problem that we often encounter in our work: “Container killed by YARN for exceeding memory limits”. What causes this problem? What is the difference between this problem and OOM? What is the relationship between this problem and the memory structure of Spark Executor? Today, let’s explore these three questions.</p>
<p>First of all, a Spark cluster will start two types of JVM processes, the Driver and the Executor. The former is the main control process, responsible for creating the Spark context, submitting Spark jobs, and transforming jobs into compute tasks. It also coordinates task scheduling among the various Executor processes. The latter is responsible for executing specific compute tasks on worker nodes, returning results to the Driver, and providing storage functionality for RDDs that need to be persisted. Since the memory management of the Driver is relatively simple, this article mainly analyzes the memory management of the Executor. Hereafter, the term “Spark memory” refers specifically to the memory of the Executor. [Note that this article targets Spark version 2.x, deployed in YARN mode.]</p>
<p><em>the memory layout of the entire Executor side is shown in the following figure.</em></p>
<p><img src="/blog/2023/03/19/spark-executor-memory-introduction/executorMemoryOverview.jpg"></p>
<p>We can see that in the Yarn cluster management mode, Spark runs in the form of Executor Containers in NodeManager, with its maximum available memory limit specified by yarn.scheduler.maximum-allocation-mb, which we call MonitorMemory.</p>
<h2 id="Executor-Memory-Strucutre-On-Yarn-Overview"><a href="#Executor-Memory-Strucutre-On-Yarn-Overview" class="headerlink" title="Executor Memory Strucutre On Yarn Overview"></a>Executor Memory Strucutre On Yarn Overview</h2><p>The Executor memory area on yarn is divided into two parts:</p>
<ol>
<li>JVM off-heap memory<br>The size is specified by the spark.yarn.executor.memoryOverhead parameter, with a default size of executorMemory * 0.10, with a minimum of 384m. This memory is mainly used for JVM itself, strings, NIO Buffer (Direct Buffer), and other overheads. This part is for user code and Spark’s non-operable memory, and can be adjusted by tuning parameters when it is insufficient.</li>
<li>On-heap memory (Spark Executor Memory)<br>The size is configured by the –executor-memory or spark.executor.memory parameter when the Spark application starts, which is the maximum heap memory allocated by the JVM (-Xmx). To more efficiently use this part of memory, Spark has logically partitioned and managed it. We will explain the unified memory management in detail below.</li>
</ol>
<p>For Yarn clusters, there is a limit: ExecutorMemory + MemoryOverhead &lt;&#x3D; monitormemory. If the sum of ExecutorMemory and MemoryOverhead specified when the application is submitted is greater than MonitorMemory, the Executor will fail to be allocated. If the actual memory usage exceeds the upper limit threshold during runtime, the Yarn will terminate (kill) the Executor process.<br>After Spark 1.6, unified memory management was introduced, including two areas: on-heap memory and off-heap memory. The following is a detailed explanation of these two areas.<br>By default, Spark only uses on-heap memory. Spark manages on-heap memory in a logical “planning” manner. The on-heap memory area on the Executor side is logically divided into the following four areas:</p>
<h3 id="Spark-Executor-Memory"><a href="#Spark-Executor-Memory" class="headerlink" title="Spark Executor Memory"></a>Spark Executor Memory</h3><h4 id="Execution-Memory"><a href="#Execution-Memory" class="headerlink" title="Execution Memory"></a>Execution Memory</h4><p>mainly used to store temporary data during calculations such as Shuffle, Join, Sort, and Aggregation.<br>Storage Memory: mainly used to store Spark cache data such as RDD caching, unroll data, and broadcast data.<br>User Memory: mainly used to store data needed for RDD transformation operations, such as RDD dependencies.<br>Reserved Memory: system reserved memory, which is used to store Spark internal objects [300MB].</p>
<h4 id="Reserved-Memory"><a href="#Reserved-Memory" class="headerlink" title="Reserved Memory"></a>Reserved Memory</h4><p>The system reserves memory for storing Spark internal objects. Its size is hard-coded in the code and its value equals to 300MB, which cannot be modified (if in a testing environment, we can modify it using the spark.testing.reservedMemory parameter); if the memory allocated to an Executor is less than 1.5 * 300 &#x3D; 450M, the Executor will not be able to run.</p>
<h4 id="Storage-Memory"><a href="#Storage-Memory" class="headerlink" title="Storage Memory"></a>Storage Memory</h4><p>Mainly used to store Spark cache data, such as RDD caching, broadcast data, and unroll data. The memory occupancy ratio is UsableMemory * spark.memory.fraction * spark.memory.storageFraction. In Spark 2+, by default, Storage Memory and Execution Memory each account for approximately 30% of the system’s total memory (1 * 0.6 * 0.5 &#x3D; 0.3). In Unified Memory management, these two types of memory can be borrowed from each other. We will discuss the specific borrowing mechanism in the next section.</p>
<h4 id="Execution-Memory-1"><a href="#Execution-Memory-1" class="headerlink" title="Execution Memory"></a>Execution Memory</h4><p>Mainly used to store temporary data during calculations such as Shuffle, Join, Sort, and Aggregation. The memory occupancy ratio is UsableMemory * spark.memory.fraction * (1 - spark.memory.storageFraction). In Spark 2+, by default, Storage Memory and Execution Memory each account for approximately 30% of the system’s total memory (1 * 0.6 * (1 - 0.5) &#x3D; 0.3). In Unified Memory management, these two types of memory can be borrowed from each other.</p>
<h4 id="Other-x2F-User-Memory"><a href="#Other-x2F-User-Memory" class="headerlink" title="Other&#x2F;User Memory"></a>Other&#x2F;User Memory</h4><p>mainly used to store data needed for RDD transformation operations, such as RDD dependencies. The memory occupancy ratio is UsableMemory * (1 - spark.memory.fraction). In Spark 2+, it defaults to 40% of the available memory (1 * (1 - 0.6) &#x3D; 0.4).</p>
<p>There are a few interesting points here that we can elaborate on:</p>
<ol>
<li>Why is 300MB reserved memory set?<br>In the initial version of Unified Memory Management, the “Other” part of memory did not have a fixed value of 300MB, but instead used a percentage similar to static memory management, initially set to 25%. However, in practice, setting a low amount of memory (e.g. 1GB) led to OOM errors. This issue is discussed in detail here: “Make unified memory management work with small heaps”. Therefore, the “Other” part of memory was modified to reserve 300MB of memory upfront.</li>
<li>spark.memory.fraction decreased from 0.75 to 0.6<br>The initial value of spark.memory.fraction was set to 0.75, and many analyses of Unified Memory Management also described it as such. However, it was found that this value was set too high and resulted in long GC times. Spark 2.0 lowered the default value to 0.6. For more detailed discussion, see “Reduce spark.memory.fraction default to avoid overrunning old gen in JVM default config”.</li>
<li>Off-heap Memory<br>Spark 1.6 introduced Off-heap memory (see SPARK-11389). In this mode, memory is not allocated within the JVM, but instead uses Java’s unsafe API to directly request memory from the operating system, similar to C’s malloc(). This allows Spark to directly access off-heap memory, reducing unnecessary memory overhead and frequent GC scans and collections, improving processing performance. Additionally, off-heap memory can be accurately allocated and released, and the space occupied by serialized data can be accurately calculated, making it easier to manage and reducing errors compared to on-heap memory. The downside is that one must write the logic for memory allocation and release themselves.</li>
</ol>
<h2 id="Task-Memory-Manager"><a href="#Task-Memory-Manager" class="headerlink" title="Task Memory Manager"></a>Task Memory Manager</h2><p>Tasks in the Executor are executed as threads and share the JVM’s resources (i.e., Execution memory). There is no strong isolation between the memory resources of tasks (i.e., tasks do not have a dedicated heap area). Therefore, it is possible for an earlier arriving task to occupy a large amount of memory, causing a later arriving task to be suspended due to insufficient memory.</p>
<p>In Spark’s task memory management, a HashMap is used to store the mapping between tasks and their memory consumption. The amount of memory that each task can occupy is half to one over n of the potential available computing memory (potential available computing memory is the initial computing memory plus preemptive storage memory). When the remaining memory is less than half to one over n, the task will be suspended until other tasks release execution memory, and the memory lower limit of half to one over n is satisfied, and the task is awakened. Here, n is the number of active tasks in the current Executor.</p>
<p>For example, if the Execution memory size is 10GB and there are 5 running tasks in the current Executor, the range of memory that this task can apply for is from 10 &#x2F; (2 * 5) to 10 &#x2F; 5, which is 1GB to 2GB.</p>
<p>During task execution, if more memory is needed, it will be requested. If there is available memory, the request will be automatically successful; otherwise, an OutOfMemoryError will be thrown. The maximum number of tasks that can run simultaneously in each Executor is determined by the number of CPU cores N allocated by the Executor and the number of CPU cores C required by each task. Specifically:</p>
<p>N &#x3D; spark.executor.cores<br>C &#x3D; spark.task.cpus</p>
<p>Therefore, the maximum task parallelism of each Executor can be represented as TP &#x3D; N &#x2F; C. The value of C depends on the application type, and most applications can use the default value of 1. Thus, the main factor affecting the maximum task parallelism (i.e., the maximum number of active tasks) in the Executor is N.</p>
<p>Based on the memory usage characteristics of tasks, the Executor memory model described above can be simply abstracted as the following diagram:</p>
<p><img src="/blog/2023/03/19/spark-executor-memory-introduction/unifiedMemory.jpg"></p>
<h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>To better understand the use of on-heap and off-heap memory mentioned above, here is a simple example.<br>3.1. Only On-Heap Memory Used<br>We have submitted a Spark job with the following memory configuration:<br><code>--executor-memory 18g.</code></p>
<p>As we have not set the <code>spark.memory.fraction</code> and <code>spark.memory.storageFraction</code> parameters, we can see from the Spark UI that the available Storage Memory is displayed as follows:</p>
<p><img src="/blog/2023/03/19/spark-executor-memory-introduction/sparkHistoryUi.jpg"></p>
<p>From the image above, we can see that the available Storage Memory is 10.1GB. How was this number obtained? According to the previous rule, we can perform the following calculations:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemMemory = spark.executor.memory</span><br><span class="line">reservedMemory = 300MB</span><br><span class="line">usableMemory = systemMemory - reservedMemory</span><br><span class="line">StorageMemory= usableMemory * spark.memory.fraction * spark.memory.storageFraction</span><br></pre></td></tr></table></figure>

<p>If we substitute the values, we get the following:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemMemory = 18Gb = 19327352832 bytes</span><br><span class="line">reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800</span><br><span class="line">usableMemory = systemMemory - reservedMemory = 19327352832 - 314572800 = 19012780032</span><br><span class="line">StorageMemory = usableMemory * spark.memory.fraction * spark.memory.storageFraction = 19012780032 * 0.6 * 0.5 = 5703834009.6 = 5.312109375GB</span><br></pre></td></tr></table></figure>
<p>The value of 10.1GB displayed on the Spark UI does not match the value we obtained. Why is that? This is because the available Storage Memory displayed on the Spark UI is actually the sum of the Execution Memory and Storage Memory, i.e., usableMemory * spark.memory.fraction:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">StorageMemory = usableMemory * spark.memory.fraction = 19012780032 * 0.6 = 11407668019.2 = 10.62421GB</span><br></pre></td></tr></table></figure>
<p>This value is also incorrect because even though we have set –executor-memory 18g, the memory available to Spark’s Executor is not as large as this. It is only 17179869184 bytes, which is obtained from Runtime.getRuntime.maxMemory. Therefore, we can perform the following calculations:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemMemory = 17179869184 bytes</span><br><span class="line">reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800</span><br><span class="line">usableMemory = systemMemory - reservedMemory = 17179869184 - 314572800 = 16865296384</span><br><span class="line">StorageMemory= usableMemory * spark.memory.fraction = 16865296384 * 0.6 = 9.42421875 GB</span><br></pre></td></tr></table></figure>
<p>When we convert 16865296384 * 0.6 bytes to GB by dividing it by 1024 * 1024 * 1024, we get 9.42421875 GB, which is still different from the value displayed on the UI. This is because the Spark UI converts bytes to GB by dividing them by 1000 * 1000 * 1000, as shown below:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemMemory = 17179869184 bytes</span><br><span class="line">reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800</span><br><span class="line">usableMemory = systemMemory - reservedMemory = 17179869184 - 314572800 = 16865296384</span><br><span class="line">StorageMemory = usableMemory * spark.memory.fraction = 16865296384 * 0.6 bytes = 16865296384 * 0.6 / (1000 * 1000 * 1000) = 10.1GB</span><br></pre></td></tr></table></figure>
<p>Now, the value matches. </p>
<p>We have set –executor-memory to 18g, but the memory that Spark’s executor side can get through Runtime.getRuntime.maxMemory is actually not that big, it’s only 17179869184 bytes. How is this data calculated?</p>
<p>Runtime.getRuntime.maxMemory is the maximum memory that the program can use, and its value is smaller than the actual configured executor memory value. This is because the heap portion of the memory allocation pool is divided into three parts: Eden, Survivor, and Tenured. There are two Survivor areas in these three parts, and we can only use one of them at any time. Therefore, we can use the following formula to describe it:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. ExecutorMemory = Eden + 2 * Survivor + Tenured</span><br><span class="line">2. Runtime.getRuntime.maxMemory = Eden + Survivor + Tenured</span><br></pre></td></tr></table></figure>
<p>The value of 17179869184 bytes above may be different depending on your GC configuration, but the calculation formula above is the same.</p>
<h3 id="Using-Heap-and-Off-Heap-Memory"><a href="#Using-Heap-and-Off-Heap-Memory" class="headerlink" title="Using Heap and Off-Heap Memory"></a>Using Heap and Off-Heap Memory</h3><p>Now, what if we enable off-heap memory? Our memory configurations are as follows:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.executor.memory 18g</span><br><span class="line">spark.memory.offHeap.enabled true</span><br><span class="line">spark.memory.offHeap.size 10737418240</span><br></pre></td></tr></table></figure>
<p>From the above, we can see that the off-heap memory is 10GB. Now, the available Storage Memory shown on Spark UI is 20.9GB, as follows:</p>
<p><img src="/blog/2023/03/19/spark-executor-memory-introduction/sparkMemoryOffheapMemory.jpg"></p>
<p>Actually, the available Storage Memory shown on Spark UI is the sum of heap memory and off-heap memory. The calculation formula is as follows:</p>
<ul>
<li><p>Heap</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemMemory = 17179869184 bytes</span><br><span class="line">reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800</span><br><span class="line">usableMemory = systemMemory - reservedMemory = 17179869184 - 314572800 = 16865296384</span><br><span class="line">totalOnHeapStorageMemory = usableMemory * spark.memory.fraction = 16865296384 * 0.6 = 10119177830</span><br></pre></td></tr></table></figure>
</li>
<li><p>Off-Heap</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">totalOffHeapStorageMemory = spark.memory.offHeap.size = 10737418240</span><br></pre></td></tr></table></figure></li>
<li><p>Total Storage Memory</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">StorageMemory = totalOnHeapStorageMemory + totalOffHeapStorageMemory = (10119177830 + 10737418240) bytes = (20856596070 / (1000 * 1000 * 1000)) GB = 20.9 GB</span><br></pre></td></tr></table></figure></li>
</ul>
<p>By now, we have shown you a clear picture of the memory architecture and management logic of Spark executor. Going back to the first two questions we mentioned earlier, when the executor serializes data, all memory management is actually not handled by the JVM. Therefore, continuous requests can lead to the YARN container exceeding the memory limit and being killed by YARN. And when Spark encounters OOM due to data skew issues, the OOM actually occurs inside the JVM. As a user, we can easily determine the direct cause of our problem based on the different error messages of these two OOMs, whether it is caused by the overhead memory area or the insufficient no-heap JVM memory, and thus correctly deduce the direction we need to optimize.</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/blog/2022/07/04/spark-tutorial/" rel="prev" title="Spark Basic Tuning Guid">
      <i class="fa fa-chevron-left"></i> Spark Basic Tuning Guid
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Overview"><span class="nav-number">1.</span> <span class="nav-text">Overview</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Executor-Memory-Strucutre-On-Yarn-Overview"><span class="nav-number">1.1.</span> <span class="nav-text">Executor Memory Strucutre On Yarn Overview</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-Executor-Memory"><span class="nav-number">1.1.1.</span> <span class="nav-text">Spark Executor Memory</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Execution-Memory"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">Execution Memory</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Reserved-Memory"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">Reserved Memory</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Storage-Memory"><span class="nav-number">1.1.1.3.</span> <span class="nav-text">Storage Memory</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Execution-Memory-1"><span class="nav-number">1.1.1.4.</span> <span class="nav-text">Execution Memory</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Other-x2F-User-Memory"><span class="nav-number">1.1.1.5.</span> <span class="nav-text">Other&#x2F;User Memory</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Task-Memory-Manager"><span class="nav-number">1.2.</span> <span class="nav-text">Task Memory Manager</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Example"><span class="nav-number">1.3.</span> <span class="nav-text">Example</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Using-Heap-and-Off-Heap-Memory"><span class="nav-number">1.3.1.</span> <span class="nav-text">Using Heap and Off-Heap Memory</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Lorenzo Lou</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/blog/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lorenzo Lou</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/blog/lib/anime.min.js"></script>
  <script src="/blog/lib/velocity/velocity.min.js"></script>
  <script src="/blog/lib/velocity/velocity.ui.min.js"></script>

<script src="/blog/js/utils.js"></script>

<script src="/blog/js/motion.js"></script>


<script src="/blog/js/schemes/pisces.js"></script>


<script src="/blog/js/next-boot.js"></script>




  




  
<script src="/blog/js/local-search.js"></script>













  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '686954989cace0b7e2ec',
      clientSecret: 'd06f0e8553b92cf39b567431dbeaeec90c9f59a8',
      repo        : 'blog',
      owner       : 'LorenzoLou',
      admin       : ['LorenzoLou'],
      id          : 'e08674feb29432e505f5b7c580233275',
        language: 'en',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
